{"cells":[{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<a href=\"https://www.skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DL0120ENedX/labs/Template%20for%20Instructional%20Hands-on%20Labs/images/IDSNlogo.png\" width=\"400px\" align=\"center\"></a>\n","\n","<h1 align=\"center\"><font size=\"5\">RECURRENT NETWORKS and LSTM IN DEEP LEARNING</font></h1>\n"],"metadata":{}},{"cell_type":"markdown","source":["<h2>Applying Recurrent Neural Networks/LSTM for Language Modeling</h2>\n","Hello and welcome to this part. In this notebook, we will go over the topic of Language Modelling, and create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and benchmark on the Penn Treebank dataset. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modelling.\n"],"metadata":{}},{"cell_type":"markdown","source":["<h2>The Objective</h2>\n","By now, you should have an understanding of how Recurrent Networks work -- a specialized model to process sequential data by keeping track of the \"state\" or context. In this notebook, we go over a TensorFlow code snippet for creating a model focused on <b>Language Modelling</b> -- a very relevant task that is the cornerstone of many different linguistic problems such as <b>Speech Recognition, Machine Translation and Image Captioning</b>. For this, we will be using the Penn Treebank dataset, which is an often-used dataset for benchmarking Language Modelling models.\n"],"metadata":{}},{"cell_type":"markdown","source":["<h2>Table of Contents</h2>\n","<ol>\n","    <li><a href=\"https://#language_modelling\">What exactly is Language Modelling?</a></li>\n","    <li><a href=\"https://#treebank_dataset\">The Penn Treebank dataset</a></li>\n","    <li><a href=\"https://#word_embedding\">Word Embedding</a></li>\n","    <li><a href=\"https://#building_lstm_model\">Building the LSTM model for Language Modeling</a></li>\n","    <li><a href=\"https://#ltsm\">LTSM</a></li>\n","</ol>\n","<p></p>\n","</div>\n","<br>\n"],"metadata":{}},{"cell_type":"markdown","source":["***\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<a id=\"language_modelling\"></a>\n","\n","<h2>What exactly is Language Modelling?</h2>\n","Language Modelling, to put it simply, <b>is the task of assigning probabilities to sequences of words</b>. This means that, given a context of one or a sequence of words in the language the model was trained on, the model should provide the next most probable words or sequence of words that follows from the given sequence of words the sentence. Language Modelling is one of the most important tasks in Natural Language Processing.\n","\n","<img src=\"https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png\" width=\"1080\">\n","<center><i>Example of a sentence being predicted</i></center>\n","<br><br>\n","In this example, one can see the predictions for the next word of a sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as <b>Speech Recognition, Machine Translation, Image Captioning, Text Correction</b> and many other very relevant problems. \n","\n","<img src=\"https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png\" width=\"1080\">\n","<center><i>The above example is a schema of an RNN in execution</i></center>\n","<br><br>\n","As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach <b>low levels of perplexity</b> on our desired dataset.\n","\n","For Language Modelling problems, <b>perplexity</b> is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that <b>low perplexity means a higher degree of trust in the predictions the model makes</b>. Therefore, the lower perplexity is, the better.\n"],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"treebank_dataset\"></a>\n","\n","<h2>The Penn Treebank dataset</h2>\n","Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be able to be correct at all. This means that we need a <b>large amount of data, annotated by or at least corrected by humans</b>. This is, of course, not an easy task at all.\n","\n","The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is <i>huge</i> -- there are over <b>four million and eight hundred thousand</b> annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank is commonly used as a benchmark dataset for Language Modelling.\n","\n","The dataset is divided in different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag --<code>\\<unk></code>\n",", which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence.\n","\n","<center>Example of text from the dataset we are going to use, <b>ptb.train</b></center>\n","<br><br>\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","    <center>the percentage of lung cancer deaths among the workers at the west <code>&lt;unk&gt;</code> mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said \n"," the plant which is owned by <code>&lt;unk&gt;</code> & <code>&lt;unk&gt;</code> co. was under contract with <code>&lt;unk&gt;</code> to make the cigarette filters \n"," the finding probably will support those who argue that the U.S. should regulate the class of asbestos including <code>&lt;unk&gt;</code> more <code>&lt;unk&gt;</code> than the common kind of asbestos <code>&lt;unk&gt;</code> found in most schools and other buildings dr. <code>&lt;unk&gt;</code> said</center>\n","</div>\n"],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"word_embedding\"></a>\n","\n","<h2>Word Embeddings</h2><br/>\n","\n","For better processing, in this example, we will make use of <a href=\"https://www.tensorflow.org/tutorials/word2vec/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\"><b>word embeddings</b></a>, which is <b>a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers</b>. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be a very effective task in Natural Language Processing, and is a commonplace practice. <br><br> <font size=\"4\"><strong>\n","$$Vec(\"Example\") = \\[0.02, 0.00, 0.00, 0.92, 0.30, \\ldots]$$ </strong></font> <br>\n","Word Embedding tends to group up similarly used words <i>reasonably</i> close together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and plot these words in a 2-dimensional space, we might see something like this:\n","\n","<img src=\"https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png\" width=\"800\">\n","<center><i>T-SNE Mockup with clusters marked for easier visualization</i></center>\n","<br><br>\n","As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher they are correlated. For example, \"None\" is pretty semantically close to \"Zero\", while a phrase that uses \"Italy\", you could probably also fit \"Germany\" in it, with little damage to the sentence structure. The vectorial \"closeness\" for similar words like this is a great indicator of a well-built model.\n","\n","<hr>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow\\.models.rnn</code></b> model, which includes the function for building RNNs, and <b><code>tensorflow\\.models.rnn.ptb.reader</code></b> which is the helper module for getting the input data from the dataset we just downloaded.\n","\n","If you want to learn more take a look at <https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":2,"source":["!pip install tensorflow==2.5.0rc0\r\n","!pip install numpy\r\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.5.0rc0\n","  Downloading tensorflow-2.5.0rc0-cp39-cp39-win_amd64.whl (422.5 MB)\n","Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (3.17.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.1.2)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (2.5.0.dev2021032900)\n","Requirement already satisfied: google-pasta~=0.2 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (0.2.0)\n","Requirement already satisfied: gast==0.4.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (0.4.0)\n","Requirement already satisfied: numpy~=1.19.2 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.19.5)\n","Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.34.1)\n","Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.6.3)\n","Requirement already satisfied: tensorboard~=2.4 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (2.6.0)\n","Requirement already satisfied: h5py~=3.1.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (3.1.0)\n","Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.1.0)\n","Requirement already satisfied: six~=1.15.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.15.0)\n","Collecting tf-estimator-nightly==2.5.0.dev2021032501\n","  Downloading tf_estimator_nightly-2.5.0.dev2021032501-py2.py3-none-any.whl (462 kB)\n","Requirement already satisfied: wheel~=0.35 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (0.36.2)\n","Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (3.3.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (3.7.4.3)\n","Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.12.1)\n","Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (1.12)\n","Requirement already satisfied: absl-py~=0.10 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow==2.5.0rc0) (0.13.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (56.0.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (2.26.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (2.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (1.34.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (0.4.5)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.4->tensorflow==2.5.0rc0) (1.8.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.5.0rc0) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.5.0rc0) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.5.0rc0) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.5.0rc0) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.5.0rc0) (0.4.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.5.0rc0) (2.0.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.5.0rc0) (1.26.6)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.5.0rc0) (3.2)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.5.0rc0) (2021.5.30)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.5.0rc0) (3.1.1)\n","Installing collected packages: tf-estimator-nightly, tensorflow\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed tensorflow-2.5.0rc0 tf-estimator-nightly-2.5.0.dev2021032501\n","Requirement already satisfied: numpy in c:\\users\\yisiang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.19.5)\n"]}],"metadata":{"tags":[]}},{"cell_type":"code","execution_count":3,"source":["import time\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","if not tf.__version__ == '2.2.0-rc0':\r\n","    print(tf.__version__)\r\n","    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"],"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Enabling eager execution\n","INFO:tensorflow:Enabling v2 tensorshape\n","INFO:tensorflow:Enabling resource variables\n","INFO:tensorflow:Enabling tensor equality\n","INFO:tensorflow:Enabling control flow v2\n","2.5.0-rc0\n"]},{"output_type":"error","ename":"ValueError","evalue":"please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19608/3163542389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'2.2.0-rc0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mValueError\u001b[0m: please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)"]}],"metadata":{}},{"cell_type":"markdown","source":["IMPORTANT! => Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Outout\" and wait until all output disapears. Then your changes are beeing picked up\n"],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["#!mkdir data\r\n","!mkdir data/ptb\r\n","!wget -o data/ptb/reader.py https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week3/data/ptb/reader.py\r\n","!cp data/ptb/reader.py . \r\n","\r\n"],"outputs":[{"output_type":"stream","name":"stderr","text":["The syntax of the command is incorrect.\n","wget: data/ptb/reader.py: No such file or directory\n","'cp' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"tags":[]}},{"cell_type":"code","execution_count":6,"source":["import reader"],"outputs":[],"metadata":{"tags":[]}},{"cell_type":"markdown","source":["<a id=\"building_lstm_model\"></a>\n","\n","<h2>Building the LSTM model for Language Modeling</h2>\n","Now that we know exactly what we are doing, we can start building our model using TensorFlow. The very first thing we need to do is download and extract the <code>simple-examples</code> dataset, which can be done by executing the code cell below.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":7,"source":["!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \r\n","!tar xzf simple-examples.tgz -C data/"],"outputs":[{"output_type":"stream","name":"stderr","text":["--2021-08-22 13:47:46--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n","Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 2001:67c:1220:809::93e5:917, 147.229.9.23\n","Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|2001:67c:1220:809::93e5:917|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 34869662 (33M) [application/x-gtar]\n","Saving to: 'simple-examples.tgz'\n","\n","     0K .......... .......... .......... .......... ..........  0% 88.5K 6m24s\n","    50K .......... .......... .......... .......... ..........  0%  278K 4m13s\n","   100K .......... .......... .......... .......... ..........  0%  162K 3m58s\n","   150K .......... .......... .......... .......... ..........  0%  284K 3m28s\n","   200K .......... .......... .......... .......... ..........  0%  284K 3m10s\n","   250K .......... .......... .......... .......... ..........  0% 1.10M 2m43s\n","   300K .......... .......... .......... .......... ..........  1%  291K 2m36s\n","   350K .......... .......... .......... .......... ..........  1%  366K 2m28s\n","   400K .......... .......... .......... .......... ..........  1%  969K 2m15s\n","   450K .......... .......... .......... .......... ..........  1%  313K 2m12s\n","   500K .......... .......... .......... .......... ..........  1% 3.09M 2m1s\n","   550K .......... .......... .......... .......... ..........  1%  402K 1m58s\n","   600K .......... .......... .......... .......... ..........  1% 1.23M 1m50s\n","   650K .......... .......... .......... .......... ..........  2%  379K 1m49s\n","   700K .......... .......... .......... .......... ..........  2% 1.23M 1m43s\n","   750K .......... .......... .......... .......... ..........  2% 11.5M 97s\n","   800K .......... .......... .......... .......... ..........  2%  378K 96s\n","   850K .......... .......... .......... .......... ..........  2% 13.8M 91s\n","   900K .......... .......... .......... .......... ..........  2% 1.22M 87s\n","   950K .......... .......... .......... .......... ..........  2% 4.07M 83s\n","  1000K .......... .......... .......... .......... ..........  3%  414K 83s\n","  1050K .......... .......... .......... .......... ..........  3% 1.37M 80s\n","  1100K .......... .......... .......... .......... ..........  3% 4.30M 77s\n","  1150K .......... .......... .......... .......... ..........  3% 6.01M 74s\n","  1200K .......... .......... .......... .......... ..........  3%  401K 74s\n","  1250K .......... .......... .......... .......... ..........  3% 1.53M 72s\n","  1300K .......... .......... .......... .......... ..........  3% 6.13M 69s\n","  1350K .......... .......... .......... .......... ..........  4% 5.94M 67s\n","  1400K .......... .......... .......... .......... ..........  4%  401K 67s\n","  1450K .......... .......... .......... .......... ..........  4% 6.25M 65s\n","  1500K .......... .......... .......... .......... ..........  4% 1.75M 63s\n","  1550K .......... .......... .......... .......... ..........  4% 4.37M 62s\n","  1600K .......... .......... .......... .......... ..........  4% 11.0M 60s\n","  1650K .......... .......... .......... .......... ..........  4%  970K 59s\n","  1700K .......... .......... .......... .......... ..........  5%  620K 59s\n","  1750K .......... .......... .......... .......... ..........  5% 1.80M 57s\n","  1800K .......... .......... .......... .......... ..........  5% 6.13M 56s\n","  1850K .......... .......... .......... .......... ..........  5% 5.78M 54s\n","  1900K .......... .......... .......... .......... ..........  5% 11.7M 53s\n","  1950K .......... .......... .......... .......... ..........  5%  419K 53s\n","  2000K .......... .......... .......... .......... ..........  6% 1.52M 53s\n","  2050K .......... .......... .......... .......... ..........  6% 40.1M 51s\n","  2100K .......... .......... .......... .......... ..........  6% 6.21M 50s\n","  2150K .......... .......... .......... .......... ..........  6% 4.55M 49s\n","  2200K .......... .......... .......... .......... ..........  6% 11.9M 48s\n","  2250K .......... .......... .......... .......... ..........  6% 1.01M 48s\n","  2300K .......... .......... .......... .......... ..........  6%  657K 47s\n","  2350K .......... .......... .......... .......... ..........  7% 1.74M 47s\n","  2400K .......... .......... .......... .......... ..........  7% 12.6M 46s\n","  2450K .......... .......... .......... .......... ..........  7% 5.84M 45s\n","  2500K .......... .......... .......... .......... ..........  7% 6.61M 44s\n","  2550K .......... .......... .......... .......... ..........  7% 10.5M 43s\n","  2600K .......... .......... .......... .......... ..........  7% 1.02M 43s\n","  2650K .......... .......... .......... .......... ..........  7%  693K 43s\n","  2700K .......... .......... .......... .......... ..........  8% 6.11M 42s\n","  2750K .......... .......... .......... .......... ..........  8% 2.02M 42s\n","  2800K .......... .......... .......... .......... ..........  8% 11.3M 41s\n","  2850K .......... .......... .......... .......... ..........  8% 6.60M 40s\n","  2900K .......... .......... .......... .......... ..........  8% 6.35M 39s\n","  2950K .......... .......... .......... .......... ..........  8% 9.71M 39s\n","  3000K .......... .......... .......... .......... ..........  8% 1.13M 39s\n","  3050K .......... .......... .......... .......... ..........  9%  652K 39s\n","  3100K .......... .......... .......... .......... ..........  9% 12.6M 38s\n","  3150K .......... .......... .......... .......... ..........  9% 2.42M 38s\n","  3200K .......... .......... .......... .......... ..........  9% 5.28M 37s\n","  3250K .......... .......... .......... .......... ..........  9% 16.2M 36s\n","  3300K .......... .......... .......... .......... ..........  9% 3.22M 36s\n","  3350K .......... .......... .......... .......... ..........  9% 35.7M 35s\n","  3400K .......... .......... .......... .......... .......... 10% 13.4M 35s\n","  3450K .......... .......... .......... .......... .......... 10% 1.05M 35s\n","  3500K .......... .......... .......... .......... .......... 10%  685K 35s\n","  3550K .......... .......... .......... .......... .......... 10% 6.54M 34s\n","  3600K .......... .......... .......... .......... .......... 10% 2.51M 34s\n","  3650K .......... .......... .......... .......... .......... 10% 9.41M 33s\n","  3700K .......... .......... .......... .......... .......... 11% 6.68M 33s\n","  3750K .......... .......... .......... .......... .......... 11% 4.01M 33s\n","  3800K .......... .......... .......... .......... .......... 11% 13.9M 32s\n","  3850K .......... .......... .......... .......... .......... 11% 40.6M 32s\n","  3900K .......... .......... .......... .......... .......... 11%  421K 32s\n","  3950K .......... .......... .......... .......... .......... 11% 24.0M 32s\n","  4000K .......... .......... .......... .......... .......... 11% 2.68M 31s\n","  4050K .......... .......... .......... .......... .......... 12% 6.38M 31s\n","  4100K .......... .......... .......... .......... .......... 12% 9.69M 31s\n","  4150K .......... .......... .......... .......... .......... 12% 4.34M 30s\n","  4200K .......... .......... .......... .......... .......... 12% 5.78M 30s\n","  4250K .......... .......... .......... .......... .......... 12% 14.4M 30s\n","  4300K .......... .......... .......... .......... .......... 12% 39.0M 29s\n","  4350K .......... .......... .......... .......... .......... 12%  421K 30s\n","  4400K .......... .......... .......... .......... .......... 13% 34.2M 29s\n","  4450K .......... .......... .......... .......... .......... 13% 2.08M 29s\n","  4500K .......... .......... .......... .......... .......... 13% 12.5M 29s\n","  4550K .......... .......... .......... .......... .......... 13% 4.07M 28s\n","  4600K .......... .......... .......... .......... .......... 13% 6.61M 28s\n","  4650K .......... .......... .......... .......... .......... 13% 10.9M 28s\n","  4700K .......... .......... .......... .......... .......... 13% 14.9M 27s\n","  4750K .......... .......... .......... .......... .......... 14% 40.6M 27s\n","  4800K .......... .......... .......... .......... .......... 14%  418K 28s\n","  4850K .......... .......... .......... .......... .......... 14% 14.9M 27s\n","  4900K .......... .......... .......... .......... .......... 14% 2.40M 27s\n","  4950K .......... .......... .......... .......... .......... 14% 6.12M 27s\n","  5000K .......... .......... .......... .......... .......... 14% 3.97M 26s\n","  5050K .......... .......... .......... .......... .......... 14% 6.58M 26s\n","  5100K .......... .......... .......... .......... .......... 15% 11.2M 26s\n","  5150K .......... .......... .......... .......... .......... 15% 34.3M 26s\n","  5200K .......... .......... .......... .......... .......... 15% 1.41M 26s\n","  5250K .......... .......... .......... .......... .......... 15%  592K 26s\n","  5300K .......... .......... .......... .......... .......... 15% 14.8M 25s\n","  5350K .......... .......... .......... .......... .......... 15% 2.02M 25s\n","  5400K .......... .......... .......... .......... .......... 16% 6.34M 25s\n","  5450K .......... .......... .......... .......... .......... 16% 5.84M 25s\n","  5500K .......... .......... .......... .......... .......... 16% 4.18M 25s\n","  5550K .......... .......... .......... .......... .......... 16% 35.1M 24s\n","  5600K .......... .......... .......... .......... .......... 16% 42.0M 24s\n","  5650K .......... .......... .......... .......... .......... 16% 1.17M 24s\n","  5700K .......... .......... .......... .......... .......... 16%  653K 24s\n","  5750K .......... .......... .......... .......... .......... 17% 3.96M 24s\n","  5800K .......... .......... .......... .......... .......... 17% 2.60M 24s\n","  5850K .......... .......... .......... .......... .......... 17% 6.06M 24s\n","  5900K .......... .......... .......... .......... .......... 17% 10.2M 23s\n","  5950K .......... .......... .......... .......... .......... 17% 4.40M 23s\n","  6000K .......... .......... .......... .......... .......... 17% 37.7M 23s\n","  6050K .......... .......... .......... .......... .......... 17% 1.36M 23s\n","  6100K .......... .......... .......... .......... .......... 18% 1.04M 23s\n","  6150K .......... .......... .......... .......... .......... 18% 1.30M 23s\n","  6200K .......... .......... .......... .......... .......... 18% 4.86M 23s\n","  6250K .......... .......... .......... .......... .......... 18% 2.40M 23s\n","  6300K .......... .......... .......... .......... .......... 18% 6.00M 22s\n","  6350K .......... .......... .......... .......... .......... 18% 11.3M 22s\n","  6400K .......... .......... .......... .......... .......... 18% 4.24M 22s\n","  6450K .......... .......... .......... .......... .......... 19% 1.53M 22s\n","  6500K .......... .......... .......... .......... .......... 19% 3.85M 22s\n","  6550K .......... .......... .......... .......... .......... 19%  662K 22s\n","  6600K .......... .......... .......... .......... .......... 19% 35.8M 22s\n","  6650K .......... .......... .......... .......... .......... 19% 4.64M 22s\n","  6700K .......... .......... .......... .......... .......... 19% 1.75M 21s\n","  6750K .......... .......... .......... .......... .......... 19% 36.9M 21s\n","  6800K .......... .......... .......... .......... .......... 20% 10.1M 21s\n","  6850K .......... .......... .......... .......... .......... 20% 4.72M 21s\n","  6900K .......... .......... .......... .......... .......... 20% 1.38M 21s\n","  6950K .......... .......... .......... .......... .......... 20% 1.01M 21s\n","  7000K .......... .......... .......... .......... .......... 20% 1.34M 21s\n","  7050K .......... .......... .......... .......... .......... 20% 35.3M 21s\n","  7100K .......... .......... .......... .......... .......... 20% 1.87M 21s\n","  7150K .......... .......... .......... .......... .......... 21% 4.00M 20s\n","  7200K .......... .......... .......... .......... .......... 21% 34.5M 20s\n","  7250K .......... .......... .......... .......... .......... 21% 6.91M 20s\n","  7300K .......... .......... .......... .......... .......... 21% 6.34M 20s\n","  7350K .......... .......... .......... .......... .......... 21% 1.11M 20s\n","  7400K .......... .......... .......... .......... .......... 21%  660K 20s\n","  7450K .......... .......... .......... .......... .......... 22% 40.2M 20s\n","  7500K .......... .......... .......... .......... .......... 22% 15.9M 20s\n","  7550K .......... .......... .......... .......... .......... 22% 1.76M 20s\n","  7600K .......... .......... .......... .......... .......... 22% 5.65M 20s\n","  7650K .......... .......... .......... .......... .......... 22% 11.6M 19s\n","  7700K .......... .......... .......... .......... .......... 22% 12.2M 19s\n","  7750K .......... .......... .......... .......... .......... 22% 1.38M 19s\n","  7800K .......... .......... .......... .......... .......... 23% 3.10M 19s\n","  7850K .......... .......... .......... .......... .......... 23%  657K 19s\n","  7900K .......... .......... .......... .......... .......... 23% 34.7M 19s\n","  7950K .......... .......... .......... .......... .......... 23% 19.1M 19s\n","  8000K .......... .......... .......... .......... .......... 23% 1.72M 19s\n","  8050K .......... .......... .......... .......... .......... 23% 5.85M 19s\n","  8100K .......... .......... .......... .......... .......... 23% 11.7M 19s\n","  8150K .......... .......... .......... .......... .......... 24% 12.9M 18s\n","  8200K .......... .......... .......... .......... .......... 24% 1.24M 18s\n","  8250K .......... .......... .......... .......... .......... 24% 3.95M 18s\n","  8300K .......... .......... .......... .......... .......... 24%  660K 18s\n","  8350K .......... .......... .......... .......... .......... 24% 25.7M 18s\n","  8400K .......... .......... .......... .......... .......... 24% 8.23M 18s\n","  8450K .......... .......... .......... .......... .......... 24% 1.73M 18s\n","  8500K .......... .......... .......... .......... .......... 25% 5.72M 18s\n","  8550K .......... .......... .......... .......... .......... 25% 38.9M 18s\n","  8600K .......... .......... .......... .......... .......... 25% 4.64M 18s\n","  8650K .......... .......... .......... .......... .......... 25% 1.44M 18s\n","  8700K .......... .......... .......... .......... .......... 25% 4.81M 18s\n","  8750K .......... .......... .......... .......... .......... 25%  657K 18s\n","  8800K .......... .......... .......... .......... .......... 25% 38.2M 18s\n","  8850K .......... .......... .......... .......... .......... 26% 7.65M 17s\n","  8900K .......... .......... .......... .......... .......... 26% 1.69M 17s\n","  8950K .......... .......... .......... .......... .......... 26% 5.47M 17s\n","  9000K .......... .......... .......... .......... .......... 26% 22.4M 17s\n","  9050K .......... .......... .......... .......... .......... 26% 5.97M 17s\n","  9100K .......... .......... .......... .......... .......... 26% 1.35M 17s\n","  9150K .......... .......... .......... .......... .......... 27%  661K 17s\n","  9200K .......... .......... .......... .......... .......... 27% 6.03M 17s\n","  9250K .......... .......... .......... .......... .......... 27% 13.6M 17s\n","  9300K .......... .......... .......... .......... .......... 27% 11.4M 17s\n","  9350K .......... .......... .......... .......... .......... 27% 1.53M 17s\n","  9400K .......... .......... .......... .......... .......... 27% 9.61M 17s\n","  9450K .......... .......... .......... .......... .......... 27% 16.6M 16s\n","  9500K .......... .......... .......... .......... .......... 28% 1.35M 16s\n","  9550K .......... .......... .......... .......... .......... 28% 4.23M 16s\n","  9600K .......... .......... .......... .......... .......... 28%  688K 16s\n","  9650K .......... .......... .......... .......... .......... 28% 6.00M 16s\n","  9700K .......... .......... .......... .......... .......... 28% 14.1M 16s\n","  9750K .......... .......... .......... .......... .......... 28% 12.2M 16s\n","  9800K .......... .......... .......... .......... .......... 28% 1.37M 16s\n","  9850K .......... .......... .......... .......... .......... 29% 40.6M 16s\n","  9900K .......... .......... .......... .......... .......... 29% 6.91M 16s\n","  9950K .......... .......... .......... .......... .......... 29% 1.37M 16s\n"," 10000K .......... .......... .......... .......... .......... 29% 4.01M 16s\n"," 10050K .......... .......... .......... .......... .......... 29%  728K 16s\n"," 10100K .......... .......... .......... .......... .......... 29% 5.77M 16s\n"," 10150K .......... .......... .......... .......... .......... 29% 12.8M 16s\n"," 10200K .......... .......... .......... .......... .......... 30% 6.64M 16s\n"," 10250K .......... .......... .......... .......... .......... 30% 1.54M 16s\n"," 10300K .......... .......... .......... .......... .......... 30% 32.6M 15s\n"," 10350K .......... .......... .......... .......... .......... 30% 4.76M 15s\n"," 10400K .......... .......... .......... .......... .......... 30% 1.52M 15s\n"," 10450K .......... .......... .......... .......... .......... 30%  625K 15s\n"," 10500K .......... .......... .......... .......... .......... 30% 5.71M 15s\n"," 10550K .......... .......... .......... .......... .......... 31% 11.4M 15s\n"," 10600K .......... .......... .......... .......... .......... 31% 14.9M 15s\n"," 10650K .......... .......... .......... .......... .......... 31% 12.1M 15s\n"," 10700K .......... .......... .......... .......... .......... 31% 1.50M 15s\n"," 10750K .......... .......... .......... .......... .......... 31% 38.5M 15s\n"," 10800K .......... .......... .......... .......... .......... 31% 4.79M 15s\n"," 10850K .......... .......... .......... .......... .......... 32% 1.37M 15s\n"," 10900K .......... .......... .......... .......... .......... 32%  656K 15s\n"," 10950K .......... .......... .......... .......... .......... 32% 4.13M 15s\n"," 11000K .......... .......... .......... .......... .......... 32% 36.7M 15s\n"," 11050K .......... .......... .......... .......... .......... 32% 16.2M 15s\n"," 11100K .......... .......... .......... .......... .......... 32% 2.49M 15s\n"," 11150K .......... .......... .......... .......... .......... 32% 2.70M 14s\n"," 11200K .......... .......... .......... .......... .......... 33% 8.09M 14s\n"," 11250K .......... .......... .......... .......... .......... 33% 1.75M 14s\n"," 11300K .......... .......... .......... .......... .......... 33% 3.06M 14s\n"," 11350K .......... .......... .......... .......... .......... 33%  625K 14s\n"," 11400K .......... .......... .......... .......... .......... 33% 11.4M 14s\n"," 11450K .......... .......... .......... .......... .......... 33% 13.2M 14s\n"," 11500K .......... .......... .......... .......... .......... 33% 39.8M 14s\n"," 11550K .......... .......... .......... .......... .......... 34% 2.54M 14s\n"," 11600K .......... .......... .......... .......... .......... 34% 2.94M 14s\n"," 11650K .......... .......... .......... .......... .......... 34% 6.51M 14s\n"," 11700K .......... .......... .......... .......... .......... 34% 1.56M 14s\n"," 11750K .......... .......... .......... .......... .......... 34% 3.97M 14s\n"," 11800K .......... .......... .......... .......... .......... 34%  620K 14s\n"," 11850K .......... .......... .......... .......... .......... 34% 13.8M 14s\n"," 11900K .......... .......... .......... .......... .......... 35% 13.2M 14s\n"," 11950K .......... .......... .......... .......... .......... 35% 38.7M 14s\n"," 12000K .......... .......... .......... .......... .......... 35% 1.59M 14s\n"," 12050K .......... .......... .......... .......... .......... 35% 10.1M 13s\n"," 12100K .......... .......... .......... .......... .......... 35% 6.29M 13s\n"," 12150K .......... .......... .......... .......... .......... 35% 1.75M 13s\n"," 12200K .......... .......... .......... .......... .......... 35%  570K 13s\n"," 12250K .......... .......... .......... .......... .......... 36% 5.48M 13s\n"," 12300K .......... .......... .......... .......... .......... 36% 16.1M 13s\n"," 12350K .......... .......... .......... .......... .......... 36% 12.7M 13s\n"," 12400K .......... .......... .......... .......... .......... 36% 12.6M 13s\n"," 12450K .......... .......... .......... .......... .......... 36% 1.73M 13s\n"," 12500K .......... .......... .......... .......... .......... 36% 5.68M 13s\n"," 12550K .......... .......... .......... .......... .......... 37% 6.41M 13s\n"," 12600K .......... .......... .......... .......... .......... 37% 1.52M 13s\n"," 12650K .......... .......... .......... .......... .......... 37%  614K 13s\n"," 12700K .......... .......... .......... .......... .......... 37% 5.79M 13s\n"," 12750K .......... .......... .......... .......... .......... 37% 5.25M 13s\n"," 12800K .......... .......... .......... .......... .......... 37%  113M 13s\n"," 12850K .......... .......... .......... .......... .......... 37% 2.70M 13s\n"," 12900K .......... .......... .......... .......... .......... 38% 3.45M 13s\n"," 12950K .......... .......... .......... .......... .......... 38% 10.5M 13s\n"," 13000K .......... .......... .......... .......... .......... 38% 1.74M 13s\n"," 13050K .......... .......... .......... .......... .......... 38% 2.85M 13s\n"," 13100K .......... .......... .......... .......... .......... 38%  676K 13s\n"," 13150K .......... .......... .......... .......... .......... 38% 5.39M 13s\n"," 13200K .......... .......... .......... .......... .......... 38% 6.68M 12s\n"," 13250K .......... .......... .......... .......... .......... 39% 12.0M 12s\n"," 13300K .......... .......... .......... .......... .......... 39% 2.97M 12s\n"," 13350K .......... .......... .......... .......... .......... 39% 3.10M 12s\n"," 13400K .......... .......... .......... .......... .......... 39% 6.32M 12s\n"," 13450K .......... .......... .......... .......... .......... 39% 1.77M 12s\n"," 13500K .......... .......... .......... .......... .......... 39%  960K 12s\n"," 13550K .......... .......... .......... .......... .......... 39% 1.12M 12s\n"," 13600K .......... .......... .......... .......... .......... 40% 5.38M 12s\n"," 13650K .......... .......... .......... .......... .......... 40% 14.9M 12s\n"," 13700K .......... .......... .......... .......... .......... 40% 44.5M 12s\n"," 13750K .......... .......... .......... .......... .......... 40% 2.58M 12s\n"," 13800K .......... .......... .......... .......... .......... 40% 2.95M 12s\n"," 13850K .......... .......... .......... .......... .......... 40% 13.4M 12s\n"," 13900K .......... .......... .......... .......... .......... 40% 1.54M 12s\n"," 13950K .......... .......... .......... .......... .......... 41%  595K 12s\n"," 14000K .......... .......... .......... .......... .......... 41% 11.9M 12s\n"," 14050K .......... .......... .......... .......... .......... 41% 3.14M 12s\n"," 14100K .......... .......... .......... .......... .......... 41% 38.0M 12s\n"," 14150K .......... .......... .......... .......... .......... 41% 38.5M 12s\n"," 14200K .......... .......... .......... .......... .......... 41% 1.64M 12s\n"," 14250K .......... .......... .......... .......... .......... 41% 10.1M 11s\n"," 14300K .......... .......... .......... .......... .......... 42% 5.92M 11s\n"," 14350K .......... .......... .......... .......... .......... 42% 1.59M 11s\n"," 14400K .......... .......... .......... .......... .......... 42%  618K 11s\n"," 14450K .......... .......... .......... .......... .......... 42% 4.40M 11s\n"," 14500K .......... .......... .......... .......... .......... 42% 5.35M 11s\n"," 14550K .......... .......... .......... .......... .......... 42% 34.3M 11s\n"," 14600K .......... .......... .......... .......... .......... 43% 4.61M 11s\n"," 14650K .......... .......... .......... .......... .......... 43% 2.12M 11s\n"," 14700K .......... .......... .......... .......... .......... 43% 42.0M 11s\n"," 14750K .......... .......... .......... .......... .......... 43% 6.66M 11s\n"," 14800K .......... .......... .......... .......... .......... 43% 1.38M 11s\n"," 14850K .......... .......... .......... .......... .......... 43%  657K 11s\n"," 14900K .......... .......... .......... .......... .......... 43% 3.07M 11s\n"," 14950K .......... .......... .......... .......... .......... 44% 10.0M 11s\n"," 15000K .......... .......... .......... .......... .......... 44% 4.12M 11s\n"," 15050K .......... .......... .......... .......... .......... 44% 37.2M 11s\n"," 15100K .......... .......... .......... .......... .......... 44% 1.86M 11s\n"," 15150K .......... .......... .......... .......... .......... 44% 42.5M 11s\n"," 15200K .......... .......... .......... .......... .......... 44% 2.10M 11s\n"," 15250K .......... .......... .......... .......... .......... 44% 3.11M 11s\n"," 15300K .......... .......... .......... .......... .......... 45%  626K 11s\n"," 15350K .......... .......... .......... .......... .......... 45% 3.10M 11s\n"," 15400K .......... .......... .......... .......... .......... 45% 4.00M 11s\n"," 15450K .......... .......... .......... .......... .......... 45% 34.0M 11s\n"," 15500K .......... .......... .......... .......... .......... 45% 21.0M 10s\n"," 15550K .......... .......... .......... .......... .......... 45% 2.02M 10s\n"," 15600K .......... .......... .......... .......... .......... 45% 10.4M 10s\n"," 15650K .......... .......... .......... .......... .......... 46% 2.45M 10s\n"," 15700K .......... .......... .......... .......... .......... 46%  898K 10s\n"," 15750K .......... .......... .......... .......... .......... 46% 1.02M 10s\n"," 15800K .......... .......... .......... .......... .......... 46% 6.16M 10s\n"," 15850K .......... .......... .......... .......... .......... 46% 4.00M 10s\n"," 15900K .......... .......... .......... .......... .......... 46% 12.3M 10s\n"," 15950K .......... .......... .......... .......... .......... 46% 13.8M 10s\n"," 16000K .......... .......... .......... .......... .......... 47% 2.41M 10s\n"," 16050K .......... .......... .......... .......... .......... 47% 2.01M 10s\n"," 16100K .......... .......... .......... .......... .......... 47% 40.1M 10s\n"," 16150K .......... .......... .......... .......... .......... 47%  913K 10s\n"," 16200K .......... .......... .......... .......... .......... 47% 1.02M 10s\n"," 16250K .......... .......... .......... .......... .......... 47% 5.98M 10s\n"," 16300K .......... .......... .......... .......... .......... 48% 3.93M 10s\n"," 16350K .......... .......... .......... .......... .......... 48% 13.1M 10s\n"," 16400K .......... .......... .......... .......... .......... 48% 12.9M 10s\n"," 16450K .......... .......... .......... .......... .......... 48% 2.42M 10s\n"," 16500K .......... .......... .......... .......... .......... 48% 2.02M 10s\n"," 16550K .......... .......... .......... .......... .......... 48% 13.6M 10s\n"," 16600K .......... .......... .......... .......... .......... 48%  593K 10s\n"," 16650K .......... .......... .......... .......... .......... 49% 3.04M 10s\n"," 16700K .......... .......... .......... .......... .......... 49% 4.11M 10s\n"," 16750K .......... .......... .......... .......... .......... 49% 5.85M 10s\n"," 16800K .......... .......... .......... .......... .......... 49% 11.8M 10s\n"," 16850K .......... .......... .......... .......... .......... 49% 4.26M 10s\n"," 16900K .......... .......... .......... .......... .......... 49% 3.06M 9s\n"," 16950K .......... .......... .......... .......... .......... 49% 2.42M 9s\n"," 17000K .......... .......... .......... .......... .......... 50% 12.5M 9s\n"," 17050K .......... .......... .......... .......... .......... 50%  569K 9s\n"," 17100K .......... .......... .......... .......... .......... 50% 3.09M 9s\n"," 17150K .......... .......... .......... .......... .......... 50% 6.01M 9s\n"," 17200K .......... .......... .......... .......... .......... 50% 5.52M 9s\n"," 17250K .......... .......... .......... .......... .......... 50% 13.1M 9s\n"," 17300K .......... .......... .......... .......... .......... 50% 3.19M 9s\n"," 17350K .......... .......... .......... .......... .......... 51% 4.00M 9s\n"," 17400K .......... .......... .......... .......... .......... 51% 2.36M 9s\n"," 17450K .......... .......... .......... .......... .......... 51% 6.65M 9s\n"," 17500K .......... .......... .......... .......... .......... 51%  567K 9s\n"," 17550K .......... .......... .......... .......... .......... 51% 4.11M 9s\n"," 17600K .......... .......... .......... .......... .......... 51% 4.16M 9s\n"," 17650K .......... .......... .......... .......... .......... 51% 9.68M 9s\n"," 17700K .......... .......... .......... .......... .......... 52% 13.7M 9s\n"," 17750K .......... .......... .......... .......... .......... 52% 2.54M 9s\n"," 17800K .......... .......... .......... .......... .......... 52% 1.72M 9s\n"," 17850K .......... .......... .......... .......... .......... 52% 42.0M 9s\n"," 17900K .......... .......... .......... .......... .......... 52% 1.04M 9s\n"," 17950K .......... .......... .......... .......... .......... 52% 1.02M 9s\n"," 18000K .......... .......... .......... .......... .......... 53% 2.94M 9s\n"," 18050K .......... .......... .......... .......... .......... 53% 4.34M 9s\n"," 18100K .......... .......... .......... .......... .......... 53% 37.2M 9s\n"," 18150K .......... .......... .......... .......... .......... 53% 15.0M 9s\n"," 18200K .......... .......... .......... .......... .......... 53% 2.52M 9s\n"," 18250K .......... .......... .......... .......... .......... 53% 1.72M 9s\n"," 18300K .......... .......... .......... .......... .......... 53% 37.3M 8s\n"," 18350K .......... .......... .......... .......... .......... 54%  991K 8s\n"," 18400K .......... .......... .......... .......... .......... 54% 1.01M 8s\n"," 18450K .......... .......... .......... .......... .......... 54% 1.96M 8s\n"," 18500K .......... .......... .......... .......... .......... 54%  279M 8s\n"," 18550K .......... .......... .......... .......... .......... 54%  168M 8s\n"," 18600K .......... .......... .......... .......... .......... 54% 7.34M 8s\n"," 18650K .......... .......... .......... .......... .......... 54% 3.04M 8s\n"," 18700K .......... .......... .......... .......... .......... 55% 1.73M 8s\n"," 18750K .......... .......... .......... .......... .......... 55% 14.1M 8s\n"," 18800K .......... .......... .......... .......... .......... 55% 1.01M 8s\n"," 18850K .......... .......... .......... .......... .......... 55% 1.02M 8s\n"," 18900K .......... .......... .......... .......... .......... 55% 2.47M 8s\n"," 18950K .......... .......... .......... .......... .......... 55% 38.1M 8s\n"," 19000K .......... .......... .......... .......... .......... 55% 12.3M 8s\n"," 19050K .......... .......... .......... .......... .......... 56% 6.26M 8s\n"," 19100K .......... .......... .......... .......... .......... 56% 3.17M 8s\n"," 19150K .......... .......... .......... .......... .......... 56% 1.72M 8s\n"," 19200K .......... .......... .......... .......... .......... 56% 1.12M 8s\n"," 19250K .......... .......... .......... .......... .......... 56% 6.23M 8s\n"," 19300K .......... .......... .......... .......... .......... 56% 1.02M 8s\n"," 19350K .......... .......... .......... .......... .......... 56% 2.47M 8s\n"," 19400K .......... .......... .......... .......... .......... 57% 9.45M 8s\n"," 19450K .......... .......... .......... .......... .......... 57% 36.5M 8s\n"," 19500K .......... .......... .......... .......... .......... 57% 7.54M 8s\n"," 19550K .......... .......... .......... .......... .......... 57% 2.53M 8s\n"," 19600K .......... .......... .......... .......... .......... 57% 1.99M 8s\n"," 19650K .......... .......... .......... .......... .......... 57% 1.12M 8s\n"," 19700K .......... .......... .......... .......... .......... 57% 4.15M 8s\n"," 19750K .......... .......... .......... .......... .......... 58%  889K 8s\n"," 19800K .......... .......... .......... .......... .......... 58% 10.3M 8s\n"," 19850K .......... .......... .......... .......... .......... 58% 4.32M 8s\n"," 19900K .......... .......... .......... .......... .......... 58% 38.6M 7s\n"," 19950K .......... .......... .......... .......... .......... 58% 10.5M 7s\n"," 20000K .......... .......... .......... .......... .......... 58% 2.75M 7s\n"," 20050K .......... .......... .......... .......... .......... 59%  733K 7s\n"," 20100K .......... .......... .......... .......... .......... 59% 38.5M 7s\n"," 20150K .......... .......... .......... .......... .......... 59% 1.14M 7s\n"," 20200K .......... .......... .......... .......... .......... 59% 1.76M 7s\n"," 20250K .......... .......... .......... .......... .......... 59% 46.9M 7s\n"," 20300K .......... .......... .......... .......... .......... 59% 3.24M 7s\n"," 20350K .......... .......... .......... .......... .......... 59% 42.2M 7s\n"," 20400K .......... .......... .......... .......... .......... 60% 14.2M 7s\n"," 20450K .......... .......... .......... .......... .......... 60% 3.10M 7s\n"," 20500K .......... .......... .......... .......... .......... 60%  737K 7s\n"," 20550K .......... .......... .......... .......... .......... 60% 35.5M 7s\n"," 20600K .......... .......... .......... .......... .......... 60% 1.05M 7s\n"," 20650K .......... .......... .......... .......... .......... 60% 2.06M 7s\n"," 20700K .......... .......... .......... .......... .......... 60% 12.2M 7s\n"," 20750K .......... .......... .......... .......... .......... 61% 4.03M 7s\n"," 20800K .......... .......... .......... .......... .......... 61% 12.7M 7s\n"," 20850K .......... .......... .......... .......... .......... 61% 38.4M 7s\n"," 20900K .......... .......... .......... .......... .......... 61% 2.53M 7s\n"," 20950K .......... .......... .......... .......... .......... 61%  778K 7s\n"," 21000K .......... .......... .......... .......... .......... 61% 35.8M 7s\n"," 21050K .......... .......... .......... .......... .......... 61% 1005K 7s\n"," 21100K .......... .......... .......... .......... .......... 62% 2.43M 7s\n"," 21150K .......... .......... .......... .......... .......... 62% 6.42M 7s\n"," 21200K .......... .......... .......... .......... .......... 62% 4.05M 7s\n"," 21250K .......... .......... .......... .......... .......... 62% 32.9M 7s\n"," 21300K .......... .......... .......... .......... .......... 62% 40.0M 7s\n"," 21350K .......... .......... .......... .......... .......... 62% 1.44M 7s\n"," 21400K .......... .......... .......... .......... .......... 62% 1.01M 7s\n"," 21450K .......... .......... .......... .......... .......... 63% 13.3M 7s\n"," 21500K .......... .......... .......... .......... .......... 63%  973K 7s\n"," 21550K .......... .......... .......... .......... .......... 63% 2.41M 7s\n"," 21600K .......... .......... .......... .......... .......... 63% 12.2M 6s\n"," 21650K .......... .......... .......... .......... .......... 63% 3.76M 6s\n"," 21700K .......... .......... .......... .......... .......... 63% 35.1M 6s\n"," 21750K .......... .......... .......... .......... .......... 64% 25.1M 6s\n"," 21800K .......... .......... .......... .......... .......... 64% 1.51M 6s\n"," 21850K .......... .......... .......... .......... .......... 64% 1.01M 6s\n"," 21900K .......... .......... .......... .......... .......... 64% 6.58M 6s\n"," 21950K .......... .......... .......... .......... .......... 64% 1.03M 6s\n"," 22000K .......... .......... .......... .......... .......... 64% 2.40M 6s\n"," 22050K .......... .......... .......... .......... .......... 64% 14.6M 6s\n"," 22100K .......... .......... .......... .......... .......... 65% 3.86M 6s\n"," 22150K .......... .......... .......... .......... .......... 65% 12.8M 6s\n"," 22200K .......... .......... .......... .......... .......... 65% 8.21M 6s\n"," 22250K .......... .......... .......... .......... .......... 65%  676K 6s\n"," 22300K .......... .......... .......... .......... .......... 65% 47.1M 6s\n"," 22350K .......... .......... .......... .......... .......... 65% 1.13M 6s\n"," 22400K .......... .......... .......... .......... .......... 65% 3.11M 6s\n"," 22450K .......... .......... .......... .......... .......... 66% 2.94M 6s\n"," 22500K .......... .......... .......... .......... .......... 66% 6.56M 6s\n"," 22550K .......... .......... .......... .......... .......... 66% 3.96M 6s\n"," 22600K .......... .......... .......... .......... .......... 66% 12.9M 6s\n"," 22650K .......... .......... .......... .......... .......... 66% 4.04M 6s\n"," 22700K .......... .......... .......... .......... .......... 66%  212K 6s\n"," 22750K .......... .......... .......... .......... .......... 66%  150M 6s\n"," 22800K .......... .......... .......... .......... .......... 67% 95.4M 6s\n"," 22850K .......... .......... .......... .......... .......... 67%  332M 6s\n"," 22900K .......... .......... .......... .......... .......... 67%  309M 6s\n"," 22950K .......... .......... .......... .......... .......... 67%  351M 6s\n"," 23000K .......... .......... .......... .......... .......... 67%  266M 6s\n"," 23050K .......... .......... .......... .......... .......... 67%  434M 6s\n"," 23100K .......... .......... .......... .......... .......... 67%  360M 6s\n"," 23150K .......... .......... .......... .......... .......... 68% 62.7K 6s\n"," 23200K .......... .......... .......... .......... .......... 68%  144K 6s\n"," 23250K .......... .......... .......... .......... .......... 68%  164K 6s\n"," 23300K .......... .......... .......... .......... .......... 68%  285K 6s\n"," 23350K .......... .......... .......... .......... .......... 68%  958K 6s\n"," 23400K .......... .......... .......... .......... .......... 68%  290K 6s\n"," 23450K .......... .......... .......... .......... .......... 69%  292K 6s\n"," 23500K .......... .......... .......... .......... .......... 69%  838K 6s\n"," 23550K .......... .......... .......... .......... .......... 69%  430K 6s\n"," 23600K .......... .......... .......... .......... .......... 69%  883K 6s\n"," 23650K .......... .......... .......... .......... .......... 69%  433K 6s\n"," 23700K .......... .......... .......... .......... .......... 69%  836K 6s\n"," 23750K .......... .......... .......... .......... .......... 69%  659K 6s\n"," 23800K .......... .......... .......... .......... .......... 70% 1.02M 6s\n"," 23850K .......... .......... .......... .......... .......... 70%  893K 6s\n"," 23900K .......... .......... .......... .......... .......... 70%  692K 6s\n"," 23950K .......... .......... .......... .......... .......... 70% 1.10M 6s\n"," 24000K .......... .......... .......... .......... .......... 70%  845K 6s\n"," 24050K .......... .......... .......... .......... .......... 70%  690K 6s\n"," 24100K .......... .......... .......... .......... .......... 70% 1.11M 6s\n"," 24150K .......... .......... .......... .......... .......... 71% 10.6M 6s\n"," 24200K .......... .......... .......... .......... .......... 71%  899K 6s\n"," 24250K .......... .......... .......... .......... .......... 71%  696K 6s\n"," 24300K .......... .......... .......... .......... .......... 71% 1.12M 6s\n"," 24350K .......... .......... .......... .......... .......... 71% 9.84M 6s\n"," 24400K .......... .......... .......... .......... .......... 71%  903K 6s\n"," 24450K .......... .......... .......... .......... .......... 71%  694K 6s\n"," 24500K .......... .......... .......... .......... .......... 72% 37.2M 6s\n"," 24550K .......... .......... .......... .......... .......... 72% 1.05M 6s\n"," 24600K .......... .......... .......... .......... .......... 72%  962K 6s\n"," 24650K .......... .......... .......... .......... .......... 72%  691K 6s\n"," 24700K .......... .......... .......... .......... .......... 72% 10.9M 6s\n"," 24750K .......... .......... .......... .......... .......... 72% 1.24M 6s\n"," 24800K .......... .......... .......... .......... .......... 72% 6.04M 6s\n"," 24850K .......... .......... .......... .......... .......... 73%  967K 6s\n"," 24900K .......... .......... .......... .......... .......... 73%  690K 6s\n"," 24950K .......... .......... .......... .......... .......... 73% 11.6M 6s\n"," 25000K .......... .......... .......... .......... .......... 73% 1.23M 6s\n"," 25050K .......... .......... .......... .......... .......... 73% 12.2M 6s\n"," 25100K .......... .......... .......... .......... .......... 73% 11.8M 5s\n"," 25150K .......... .......... .......... .......... .......... 74%  418K 5s\n"," 25200K .......... .......... .......... .......... .......... 74% 8.19M 5s\n"," 25250K .......... .......... .......... .......... .......... 74% 17.0M 5s\n"," 25300K .......... .......... .......... .......... .......... 74% 1.13M 5s\n"," 25350K .......... .......... .......... .......... .......... 74% 34.5M 5s\n"," 25400K .......... .......... .......... .......... .......... 74%  913K 5s\n"," 25450K .......... .......... .......... .......... .......... 74%  737K 5s\n"," 25500K .......... .......... .......... .......... .......... 75% 10.8M 5s\n"," 25550K .......... .......... .......... .......... .......... 75% 11.5M 5s\n"," 25600K .......... .......... .......... .......... .......... 75% 1.25M 5s\n"," 25650K .......... .......... .......... .......... .......... 75% 9.63M 5s\n"," 25700K .......... .......... .......... .......... .......... 75% 14.4M 5s\n"," 25750K .......... .......... .......... .......... .......... 75%  972K 5s\n"," 25800K .......... .......... .......... .......... .......... 75%  727K 5s\n"," 25850K .......... .......... .......... .......... .......... 76% 6.89M 5s\n"," 25900K .......... .......... .......... .......... .......... 76% 38.3M 5s\n"," 25950K .......... .......... .......... .......... .......... 76% 1.24M 5s\n"," 26000K .......... .......... .......... .......... .......... 76% 11.7M 5s\n"," 26050K .......... .......... .......... .......... .......... 76% 11.7M 5s\n"," 26100K .......... .......... .......... .......... .......... 76% 35.9M 5s\n"," 26150K .......... .......... .......... .......... .......... 76%  423K 5s\n"," 26200K .......... .......... .......... .......... .......... 77% 11.8M 5s\n"," 26250K .......... .......... .......... .......... .......... 77% 39.9M 5s\n"," 26300K .......... .......... .......... .......... .......... 77% 1.28M 5s\n"," 26350K .......... .......... .......... .......... .......... 77% 11.1M 5s\n"," 26400K .......... .......... .......... .......... .......... 77% 6.18M 5s\n"," 26450K .......... .......... .......... .......... .......... 77% 35.6M 5s\n"," 26500K .......... .......... .......... .......... .......... 77% 34.6M 5s\n"," 26550K .......... .......... .......... .......... .......... 78% 1021K 5s\n"," 26600K .......... .......... .......... .......... .......... 78%  697K 5s\n"," 26650K .......... .......... .......... .......... .......... 78% 38.6M 5s\n"," 26700K .......... .......... .......... .......... .......... 78% 13.8M 4s\n"," 26750K .......... .......... .......... .......... .......... 78% 1.24M 4s\n"," 26800K .......... .......... .......... .......... .......... 78% 37.6M 4s\n"," 26850K .......... .......... .......... .......... .......... 78% 6.87M 4s\n"," 26900K .......... .......... .......... .......... .......... 79% 39.9M 4s\n"," 26950K .......... .......... .......... .......... .......... 79% 1.06M 4s\n"," 27000K .......... .......... .......... .......... .......... 79%  690K 4s\n"," 27050K .......... .......... .......... .......... .......... 79% 55.1M 4s\n"," 27100K .......... .......... .......... .......... .......... 79% 6.82M 4s\n"," 27150K .......... .......... .......... .......... .......... 79% 35.4M 4s\n"," 27200K .......... .......... .......... .......... .......... 80% 1.28M 4s\n"," 27250K .......... .......... .......... .......... .......... 80% 38.6M 4s\n"," 27300K .......... .......... .......... .......... .......... 80% 6.73M 4s\n"," 27350K .......... .......... .......... .......... .......... 80% 38.2M 4s\n"," 27400K .......... .......... .......... .......... .......... 80%  997K 4s\n"," 27450K .......... .......... .......... .......... .......... 80%  730K 4s\n"," 27500K .......... .......... .......... .......... .......... 80% 12.8M 4s\n"," 27550K .......... .......... .......... .......... .......... 81% 10.2M 4s\n"," 27600K .......... .......... .......... .......... .......... 81% 37.3M 4s\n"," 27650K .......... .......... .......... .......... .......... 81% 1.18M 4s\n"," 27700K .......... .......... .......... .......... .......... 81% 37.8M 4s\n"," 27750K .......... .......... .......... .......... .......... 81% 12.7M 4s\n"," 27800K .......... .......... .......... .......... .......... 81% 1.04M 4s\n"," 27850K .......... .......... .......... .......... .......... 81%  695K 4s\n"," 27900K .......... .......... .......... .......... .......... 82% 39.4M 4s\n"," 27950K .......... .......... .......... .......... .......... 82% 7.05M 4s\n"," 28000K .......... .......... .......... .......... .......... 82% 38.5M 4s\n"," 28050K .......... .......... .......... .......... .......... 82% 1.59M 4s\n"," 28100K .......... .......... .......... .......... .......... 82% 4.00M 4s\n"," 28150K .......... .......... .......... .......... .......... 82% 39.6M 4s\n"," 28200K .......... .......... .......... .......... .......... 82% 14.2M 3s\n"," 28250K .......... .......... .......... .......... .......... 83% 1.03M 3s\n"," 28300K .......... .......... .......... .......... .......... 83%  739K 3s\n"," 28350K .......... .......... .......... .......... .......... 83% 9.47M 3s\n"," 28400K .......... .......... .......... .......... .......... 83% 7.00M 3s\n"," 28450K .......... .......... .......... .......... .......... 83% 35.8M 3s\n"," 28500K .......... .......... .......... .......... .......... 83% 1.42M 3s\n"," 28550K .......... .......... .......... .......... .......... 83% 5.77M 3s\n"," 28600K .......... .......... .......... .......... .......... 84% 44.5M 3s\n"," 28650K .......... .......... .......... .......... .......... 84% 13.8M 3s\n"," 28700K .......... .......... .......... .......... .......... 84% 1.04M 3s\n"," 28750K .......... .......... .......... .......... .......... 84%  693K 3s\n"," 28800K .......... .......... .......... .......... .......... 84% 34.7M 3s\n"," 28850K .......... .......... .......... .......... .......... 84% 7.10M 3s\n"," 28900K .......... .......... .......... .......... .......... 85% 1.54M 3s\n"," 28950K .......... .......... .......... .......... .......... 85% 37.0M 3s\n"," 29000K .......... .......... .......... .......... .......... 85% 4.25M 3s\n"," 29050K .......... .......... .......... .......... .......... 85% 12.3M 3s\n"," 29100K .......... .......... .......... .......... .......... 85% 33.9M 3s\n"," 29150K .......... .......... .......... .......... .......... 85% 1.02M 3s\n"," 29200K .......... .......... .......... .......... .......... 85%  713K 3s\n"," 29250K .......... .......... .......... .......... .......... 86% 12.0M 3s\n"," 29300K .......... .......... .......... .......... .......... 86% 10.7M 3s\n"," 29350K .......... .......... .......... .......... .......... 86% 1.57M 3s\n"," 29400K .......... .......... .......... .......... .......... 86% 35.1M 3s\n"," 29450K .......... .......... .......... .......... .......... 86% 3.14M 3s\n"," 29500K .......... .......... .......... .......... .......... 86% 33.5M 3s\n"," 29550K .......... .......... .......... .......... .......... 86% 42.7M 3s\n"," 29600K .......... .......... .......... .......... .......... 87% 1.00M 3s\n"," 29650K .......... .......... .......... .......... .......... 87%  696K 3s\n"," 29700K .......... .......... .......... .......... .......... 87% 11.3M 3s\n"," 29750K .......... .......... .......... .......... .......... 87% 39.2M 3s\n"," 29800K .......... .......... .......... .......... .......... 87% 1.58M 2s\n"," 29850K .......... .......... .......... .......... .......... 87% 59.9M 2s\n"," 29900K .......... .......... .......... .......... .......... 87% 3.13M 2s\n"," 29950K .......... .......... .......... .......... .......... 88% 40.0M 2s\n"," 30000K .......... .......... .......... .......... .......... 88% 35.0M 2s\n"," 30050K .......... .......... .......... .......... .......... 88% 1.00M 2s\n"," 30100K .......... .......... .......... .......... .......... 88%  696K 2s\n"," 30150K .......... .......... .......... .......... .......... 88% 11.1M 2s\n"," 30200K .......... .......... .......... .......... .......... 88% 42.0M 2s\n"," 30250K .......... .......... .......... .......... .......... 88% 1.33M 2s\n"," 30300K .......... .......... .......... .......... .......... 89%  245M 2s\n"," 30350K .......... .......... .......... .......... .......... 89% 4.57M 2s\n"," 30400K .......... .......... .......... .......... .......... 89% 1.23M 2s\n"," 30450K .......... .......... .......... .......... .......... 89% 35.1M 2s\n"," 30500K .......... .......... .......... .......... .......... 89%  679K 2s\n"," 30550K .......... .......... .......... .......... .......... 89% 5.84M 2s\n"," 30600K .......... .......... .......... .......... .......... 90% 10.0M 2s\n"," 30650K .......... .......... .......... .......... .......... 90% 34.7M 2s\n"," 30700K .......... .......... .......... .......... .......... 90% 1.31M 2s\n"," 30750K .......... .......... .......... .......... .......... 90% 40.0M 2s\n"," 30800K .......... .......... .......... .......... .......... 90% 1.01M 2s\n"," 30850K .......... .......... .......... .......... .......... 90% 60.2M 2s\n"," 30900K .......... .......... .......... .......... .......... 90% 23.1M 2s\n"," 30950K .......... .......... .......... .......... .......... 91%  416K 2s\n"," 31000K .......... .......... .......... .......... .......... 91% 38.4M 2s\n"," 31050K .......... .......... .......... .......... .......... 91% 39.3M 2s\n"," 31100K .......... .......... .......... .......... .......... 91% 36.6M 2s\n"," 31150K .......... .......... .......... .......... .......... 91% 33.2M 2s\n"," 31200K .......... .......... .......... .......... .......... 91% 6.85M 2s\n"," 31250K .......... .......... .......... .......... .......... 91% 1.21M 2s\n"," 31300K .......... .......... .......... .......... .......... 92% 14.7M 2s\n"," 31350K .......... .......... .......... .......... .......... 92% 13.4M 2s\n"," 31400K .......... .......... .......... .......... .......... 92%  426K 2s\n"," 31450K .......... .......... .......... .......... .......... 92% 84.1M 1s\n"," 31500K .......... .......... .......... .......... .......... 92%  167M 1s\n"," 31550K .......... .......... .......... .......... .......... 92% 20.5M 1s\n"," 31600K .......... .......... .......... .......... .......... 92% 38.6M 1s\n"," 31650K .......... .......... .......... .......... .......... 93% 1.05M 1s\n"," 31700K .......... .......... .......... .......... .......... 93% 35.5M 1s\n"," 31750K .......... .......... .......... .......... .......... 93% 23.6M 1s\n"," 31800K .......... .......... .......... .......... .......... 93% 11.3M 1s\n"," 31850K .......... .......... .......... .......... .......... 93%  432K 1s\n"," 31900K .......... .......... .......... .......... .......... 93% 33.8M 1s\n"," 31950K .......... .......... .......... .......... .......... 93% 37.3M 1s\n"," 32000K .......... .......... .......... .......... .......... 94% 23.2M 1s\n"," 32050K .......... .......... .......... .......... .......... 94% 16.0M 1s\n"," 32100K .......... .......... .......... .......... .......... 94% 1.10M 1s\n"," 32150K .......... .......... .......... .......... .......... 94% 11.9M 1s\n"," 32200K .......... .......... .......... .......... .......... 94% 40.4M 1s\n"," 32250K .......... .......... .......... .......... .......... 94% 7.99M 1s\n"," 32300K .......... .......... .......... .......... .......... 95%  443K 1s\n"," 32350K .......... .......... .......... .......... .......... 95% 73.8M 1s\n"," 32400K .......... .......... .......... .......... .......... 95% 35.1M 1s\n"," 32450K .......... .......... .......... .......... .......... 95% 9.84M 1s\n"," 32500K .......... .......... .......... .......... .......... 95% 21.3M 1s\n"," 32550K .......... .......... .......... .......... .......... 95% 1.15M 1s\n"," 32600K .......... .......... .......... .......... .......... 95% 12.7M 1s\n"," 32650K .......... .......... .......... .......... .......... 96% 37.5M 1s\n"," 32700K .......... .......... .......... .......... .......... 96% 1.16M 1s\n"," 32750K .......... .......... .......... .......... .......... 96%  654K 1s\n"," 32800K .......... .......... .......... .......... .......... 96% 38.6M 1s\n"," 32850K .......... .......... .......... .......... .......... 96% 19.9M 1s\n"," 32900K .......... .......... .......... .......... .......... 96% 10.7M 1s\n"," 32950K .......... .......... .......... .......... .......... 96% 37.5M 1s\n"," 33000K .......... .......... .......... .......... .......... 97% 1.14M 1s\n"," 33050K .......... .......... .......... .......... .......... 97% 12.3M 1s\n"," 33100K .......... .......... .......... .......... .......... 97% 12.6M 1s\n"," 33150K .......... .......... .......... .......... .......... 97% 1.13M 0s\n"," 33200K .......... .......... .......... .......... .......... 97%  693K 0s\n"," 33250K .......... .......... .......... .......... .......... 97% 10.9M 0s\n"," 33300K .......... .......... .......... .......... .......... 97% 13.8M 0s\n"," 33350K .......... .......... .......... .......... .......... 98% 40.2M 0s\n"," 33400K .......... .......... .......... .......... .......... 98% 1.14M 0s\n"," 33450K .......... .......... .......... .......... .......... 98% 12.4M 0s\n"," 33500K .......... .......... .......... .......... .......... 98% 21.4M 0s\n"," 33550K .......... .......... .......... .......... .......... 98% 7.94M 0s\n"," 33600K .......... .......... .......... .......... .......... 98%  700K 0s\n"," 33650K .......... .......... .......... .......... .......... 98% 1.11M 0s\n"," 33700K .......... .......... .......... .......... .......... 99% 10.8M 0s\n"," 33750K .......... .......... .......... .......... .......... 99% 35.2M 0s\n"," 33800K .......... .......... .......... .......... .......... 99% 1.26M 0s\n"," 33850K .......... .......... .......... .......... .......... 99% 12.3M 0s\n"," 33900K .......... .......... .......... .......... .......... 99% 12.7M 0s\n"," 33950K .......... .......... .......... .......... .......... 99% 13.0M 0s\n"," 34000K .......... .......... .......... .......... .......... 99% 1.51M 0s\n"," 34050K ..                                                    100% 45.9G=19s\n","\n","2021-08-22 13:48:15 (1.72 MB/s) - 'simple-examples.tgz' saved [34869662/34869662]\n","\n"]}],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"tags":[]}},{"cell_type":"markdown","source":["Additionally, for the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change those!\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":8,"source":["#Initial weight scale\r\n","init_scale = 0.1\r\n","#Initial learning rate\r\n","learning_rate = 1.0\r\n","#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\r\n","max_grad_norm = 5\r\n","#The number of layers in our model\r\n","num_layers = 2\r\n","#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\r\n","num_steps = 20\r\n","#The number of processing units (neurons) in the hidden layers\r\n","hidden_size_l1 = 256\r\n","hidden_size_l2 = 128\r\n","#The maximum number of epochs trained with the initial learning rate\r\n","max_epoch_decay_lr = 4\r\n","#The total number of epochs in training\r\n","max_epoch = 15\r\n","#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\r\n","#At 1, we ignore the Dropout Layer wrapping.\r\n","keep_prob = 1\r\n","#The decay for the learning rate\r\n","decay = 0.5\r\n","#The size for each batch of data\r\n","batch_size = 30\r\n","#The size of our vocabulary\r\n","vocab_size = 10000\r\n","embeding_vector_size= 200\r\n","#Training flag to separate training from testing\r\n","is_training = 1\r\n","#Data directory for our dataset\r\n","data_dir = \"data/simple-examples/data/\""],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["Some clarifications for LSTM architecture based on the arguments:\n","\n","Network structure:\n","\n","<ul>\n","    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n","    </li>\n","    <li>The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n","    <li>the structure is like:\n","        <ul>\n","            <li>200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output</li>\n","        </ul>\n","    </li>\n","</ul>\n","<br>\n","\n","Input layer:\n","\n","<ul>\n","    <li>The network has 200 input units.</li>\n","    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n","    </li>\n","    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n","    </li>\n","</ul>\n","<br>\n","\n","Hidden layer:\n","\n","<ul>\n","    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>\n","</ul>\n","<br>\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to apply what you just learned about language modelling to the code you see, you should be able to understand it.\n","\n","This code is adapted from the <a href=\"https://github.com/tensorflow/models?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\">PTBModel</a> example bundled with the TensorFlow source code.\n","\n","<h3>Training data</h3>\n","The story starts from data:\n","<ul>\n","    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>\n","    <li>We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$floor(\\frac{N}{b \\times h})+1=1548$$ iterations for the learner to go through all sentences once. Where N is the size of the list of words, b is batch size, and h is size of each sentence. So, the number of iterators is 1548\n","    </li>\n","    <li>Each batch data is read from train dataset of size 600, and shape of [30x20]</li>\n","</ul>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":9,"source":["# Reads the data and separates it into training data, validation data and testing data\r\n","raw_data = reader.ptb_raw_data(data_dir)\r\n","train_data, valid_data, test_data, vocab, word_to_id = raw_data"],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":10,"source":["len(train_data)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["929589"]},"metadata":{},"execution_count":10}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["def id_to_word(id_list):\r\n","    line = []\r\n","    for w in id_list:\r\n","        for word, wid in word_to_id.items():\r\n","            if wid == w:\r\n","                line.append(word)\r\n","    return line            \r\n","                \r\n","\r\n","print(id_to_word(train_data[0:100]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"]}],"metadata":{"tags":[]}},{"cell_type":"markdown","source":["Lets just read one mini-batch now and feed our network:\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":14,"source":["itera = reader.ptb_iterator(train_data, batch_size, num_steps)\r\n","first_touple = itera.__next__()\r\n","_input_data = first_touple[0]\r\n","_targets = first_touple[1]"],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":15,"source":["_input_data.shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30, 20)"]},"metadata":{},"execution_count":15}],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":16,"source":["_targets.shape"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30, 20)"]},"metadata":{},"execution_count":16}],"metadata":{}},{"cell_type":"markdown","source":["Lets look at 3 sentences of our input x:\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":17,"source":["_input_data[0:3]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n","        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n","       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n","         123,    7,  514,    2,   63,   10,  514,    8,  605],\n","       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n","          12,    3,   21,    2, 2915,    2,   12,    3,   21]])"]},"metadata":{},"execution_count":17}],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":18,"source":["print(id_to_word(_input_data[0,:]))"],"outputs":[{"output_type":"stream","name":"stdout","text":["['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n"]}],"metadata":{}},{"cell_type":"markdown","source":["<h3>Embeddings</h3>\n","We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, One-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n","\n","The embedded vectors also get updated during the training process of the deep neural network.\n","We create the embeddings for our input data. <b>embedding_vocab</b> is matrix of \\[10000x200] for all 10000 unique words.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<b>embedding_lookup()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the correspond vector in <code>embedding_dic<code>. <br>\n","It creates a \\[30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, which each row of it, is vector representing a word in the sentence.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":19,"source":["embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size,batch_input_shape=(batch_size, num_steps),trainable=True,name=\"embedding_vocab\")  "],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["# Define where to get the data for our embeddings from\r\n","inputs = embedding_layer(_input_data)\r\n","inputs"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(30, 20, 200), dtype=float32, numpy=\n","array([[[-0.01575189,  0.02046836, -0.00210208, ...,  0.03796058,\n","          0.00494336,  0.03049736],\n","        [ 0.02769399, -0.04502158, -0.03775523, ...,  0.01219122,\n","         -0.04293268, -0.04138865],\n","        [ 0.0273379 , -0.04852057, -0.00247671, ..., -0.01367446,\n","         -0.00920868, -0.04070465],\n","        ...,\n","        [ 0.02072715,  0.01771183, -0.01314598, ..., -0.04644802,\n","          0.03353329,  0.01654469],\n","        [-0.02630015,  0.02711408,  0.04527858, ..., -0.01636213,\n","          0.03932922, -0.02547002],\n","        [ 0.03296522,  0.04351456, -0.04705814, ..., -0.02522553,\n","          0.00555167, -0.00500467]],\n","\n","       [[ 0.0409306 ,  0.00329968,  0.01487737, ...,  0.01217846,\n","          0.02664513,  0.02044623],\n","        [ 0.00778172,  0.00449093,  0.01201712, ...,  0.02721239,\n","         -0.03301636, -0.00430375],\n","        [-0.00220746, -0.03483289, -0.02955051, ..., -0.03584809,\n","         -0.02532627,  0.02895227],\n","        ...,\n","        [ 0.00165246, -0.03536866,  0.04427597, ...,  0.02774811,\n","          0.00319394,  0.00518847],\n","        [ 0.0296775 ,  0.02645818, -0.02189188, ..., -0.02758963,\n","          0.031827  ,  0.04886133],\n","        [-0.02695805, -0.04992482,  0.04924029, ...,  0.04566528,\n","         -0.04149332,  0.03769555]],\n","\n","       [[-0.0331818 , -0.01362555, -0.0061021 , ...,  0.01685164,\n","          0.04219392, -0.02777616],\n","        [-0.0353291 , -0.04075367,  0.02732742, ...,  0.00523856,\n","         -0.00830241,  0.01829014],\n","        [ 0.00583333,  0.01431544, -0.04166994, ...,  0.01578781,\n","          0.04878718,  0.0009927 ],\n","        ...,\n","        [ 0.03849337, -0.00283104,  0.02431332, ...,  0.03799386,\n","         -0.02183139,  0.00281745],\n","        [-0.01018257,  0.04123062, -0.01276308, ...,  0.01518288,\n","          0.02498658,  0.00128012],\n","        [-0.00598359, -0.0159441 , -0.02412387, ...,  0.01944141,\n","         -0.00219607, -0.02176176]],\n","\n","       ...,\n","\n","       [[-0.00282959, -0.02821919,  0.04542357, ..., -0.0244669 ,\n","         -0.01493063, -0.00286703],\n","        [-0.04673031,  0.02712215, -0.02313707, ...,  0.02027961,\n","          0.03163359, -0.02085017],\n","        [-0.03955181, -0.03726565,  0.04426596, ...,  0.01511326,\n","          0.03100431,  0.01209923],\n","        ...,\n","        [ 0.00041668, -0.03586209,  0.04925317, ..., -0.02300791,\n","          0.00622599,  0.02528881],\n","        [ 0.04490883,  0.04122594, -0.0092059 , ..., -0.01516377,\n","          0.01128572, -0.0411923 ],\n","        [ 0.04620535,  0.01809465, -0.02853915, ...,  0.04119081,\n","         -0.01101638,  0.03580809]],\n","\n","       [[ 0.03350284, -0.0491316 ,  0.03095566, ..., -0.02941569,\n","          0.03651856, -0.00913728],\n","        [-0.02772211,  0.03587285,  0.00837674, ...,  0.03084979,\n","          0.00416474,  0.03820303],\n","        [ 0.00583333,  0.01431544, -0.04166994, ...,  0.01578781,\n","          0.04878718,  0.0009927 ],\n","        ...,\n","        [-0.02322686,  0.03703797, -0.03059486, ..., -0.04478484,\n","          0.04827621, -0.01760912],\n","        [ 0.00778172,  0.00449093,  0.01201712, ...,  0.02721239,\n","         -0.03301636, -0.00430375],\n","        [-0.01827686,  0.02792618,  0.03861824, ...,  0.01254301,\n","          0.0190728 ,  0.0439852 ]],\n","\n","       [[-0.03410422,  0.0455325 , -0.00193716, ..., -0.04678486,\n","          0.01619994, -0.03142202],\n","        [ 0.04361199, -0.04000485, -0.01161037, ..., -0.04454929,\n","          0.01711701,  0.03427752],\n","        [ 0.01344563,  0.01781532,  0.04595908, ...,  0.02639706,\n","         -0.01403215,  0.00860037],\n","        ...,\n","        [-0.03026807, -0.02135195,  0.01751561, ..., -0.01269077,\n","          0.02527714, -0.04881636],\n","        [ 0.03719917, -0.02480859, -0.0352016 , ...,  0.02974421,\n","         -0.02766577, -0.03775377],\n","        [-0.00268193, -0.02774384, -0.02091953, ..., -0.01115311,\n","         -0.01461011,  0.01429803]]], dtype=float32)>"]},"metadata":{},"execution_count":20}],"metadata":{}},{"cell_type":"markdown","source":["<h3>Constructing Recurrent Neural Networks</h3>\n"],"metadata":{}},{"cell_type":"markdown","source":["In this step, we create the stacked LSTM using <b>tf.keras.layers.StackedRNNCells</b>, which is a 2 layer LSTM network:\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":21,"source":["lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\r\n","lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])"],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<b>tf.keras.layers.RNN</b> creates a recurrent neural network using <b>stacked_lstm</b>.\n","\n","The input should be a Tensor of shape: \\[batch_size, max_time, embedding_vector_size], in our case it would be (30, 20, 200)\n"],"metadata":{}},{"cell_type":"code","execution_count":23,"source":["layer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Also, we initialize the states of the nework:\n","\n","<h4>_initial_state</h4>\n","\n","For each LSTM, there are 2 state matrices, c_state and m_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size \\[30x200]\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":24,"source":["init_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":25,"source":["layer.inital_state = init_state"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":26,"source":["layer.inital_state"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Variable 'Variable:0' shape=(30, 200) dtype=float32, numpy=\n","array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"]},"metadata":{},"execution_count":26}],"metadata":{}},{"cell_type":"markdown","source":["so, lets look at the outputs. The output of the stackedLSTM comes from 128 hidden_layer, and in each time step(=20), one of them get activated. we use the linear activation to map the 128 hidden layer to a \\[30X20 matrix]\n"],"metadata":{}},{"cell_type":"code","execution_count":27,"source":["outputs = layer(inputs)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":28,"source":["outputs"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(30, 20, 128), dtype=float32, numpy=\n","array([[[ 2.1068614e-03,  1.0568851e-03,  1.0628570e-03, ...,\n","         -8.0885127e-04,  9.6013799e-04,  6.0972641e-04],\n","        [ 2.5925126e-03,  1.7335924e-03,  3.4816207e-03, ...,\n","         -1.5809020e-03,  5.4717652e-04,  1.6870511e-03],\n","        [ 2.5663788e-03,  3.7109829e-03,  4.6979561e-03, ...,\n","         -2.5327883e-03, -6.3481100e-04,  1.6374388e-03],\n","        ...,\n","        [ 1.7943610e-03,  6.2109306e-03, -2.5819512e-03, ...,\n","          9.4887585e-04,  6.2506594e-04, -1.8105340e-03],\n","        [ 2.4377182e-03,  5.5514714e-03, -2.1585829e-03, ...,\n","          8.0760161e-04, -4.4298038e-04, -2.3108097e-03],\n","        [ 2.1540264e-03,  4.6600574e-03, -1.0541553e-03, ...,\n","         -6.0542148e-05, -2.1724454e-03, -1.7067424e-03]],\n","\n","       [[-1.1975959e-04, -1.1917953e-03,  3.9263320e-04, ...,\n","         -4.0975510e-04,  1.8240320e-03,  5.5979000e-04],\n","        [ 5.5724522e-04, -9.3943212e-04,  4.2216267e-04, ...,\n","         -1.3101207e-03,  2.8033799e-03,  5.5702653e-04],\n","        [ 6.7403319e-04, -6.6684712e-05, -3.2394644e-04, ...,\n","         -9.0642931e-04,  4.5798402e-03,  1.3498773e-03],\n","        ...,\n","        [-1.5830493e-03,  1.3487556e-03, -2.7934867e-03, ...,\n","          1.9055980e-03, -3.8918070e-03,  3.3799119e-04],\n","        [-5.7096756e-04,  5.6082959e-04, -4.5829033e-03, ...,\n","          2.1655592e-03, -1.6335316e-03, -1.2241751e-04],\n","        [ 4.1199857e-04,  7.6724339e-04, -5.8473595e-03, ...,\n","          2.0888173e-03,  8.3318789e-04,  6.3391845e-04]],\n","\n","       [[ 9.0926205e-04, -6.8625843e-04, -5.2643433e-04, ...,\n","         -4.2126939e-04,  7.3366298e-04, -2.7583085e-04],\n","        [ 1.0461845e-03, -7.8380038e-04,  1.5275960e-04, ...,\n","         -1.4782576e-03,  1.8146568e-03, -1.8421003e-04],\n","        [ 1.7378038e-03, -1.3885134e-03,  2.4214365e-04, ...,\n","         -1.1246504e-03,  4.9094256e-04,  7.3183811e-04],\n","        ...,\n","        [ 5.7010120e-03,  2.7464403e-04, -4.5546622e-04, ...,\n","          1.0378537e-02, -1.0329274e-03,  3.8583654e-03],\n","        [ 5.1905783e-03, -2.2108718e-04,  1.6849134e-04, ...,\n","          1.0679522e-02, -1.5206386e-03,  4.8578708e-03],\n","        [ 4.5306138e-03, -9.9041150e-04,  2.3264948e-03, ...,\n","          1.1624648e-02, -6.4764312e-04,  5.8580991e-03]],\n","\n","       ...,\n","\n","       [[-7.0870208e-04, -1.2699739e-03,  1.9327628e-05, ...,\n","         -4.3385665e-04, -1.8341156e-03, -1.4154449e-03],\n","        [-8.1561244e-04, -3.1663547e-03,  1.2293067e-03, ...,\n","         -1.4764960e-03, -3.3494390e-03, -2.7413114e-03],\n","        [-2.2087570e-03, -3.1165043e-03,  2.2472243e-03, ...,\n","         -3.0922685e-03, -2.8882597e-03, -4.3816390e-03],\n","        ...,\n","        [-6.7332981e-04,  2.2255697e-03, -6.4652163e-04, ...,\n","          7.3888716e-03, -1.8701517e-03,  7.6506743e-03],\n","        [ 1.1736142e-03,  1.5439190e-03,  5.9508003e-04, ...,\n","          7.2504128e-03, -2.9007932e-03,  7.9519898e-03],\n","        [ 2.5114783e-03,  1.2657958e-03,  7.1529008e-04, ...,\n","          6.9467444e-03, -2.9593962e-03,  7.3347567e-03]],\n","\n","       [[-1.0534113e-03,  1.5927694e-03, -1.3778490e-04, ...,\n","         -1.3883552e-04, -1.0601106e-03,  9.5704518e-04],\n","        [-2.6258067e-03,  1.8553105e-03, -4.8093958e-04, ...,\n","         -1.7507839e-03,  7.5197424e-04,  9.5232949e-04],\n","        [-2.3462099e-03,  1.2702649e-03, -1.2759621e-03, ...,\n","         -1.9998651e-03,  1.4708348e-04,  1.8592391e-03],\n","        ...,\n","        [-1.7176187e-03, -5.8069346e-03,  7.8636699e-04, ...,\n","         -6.8019084e-03, -1.4303614e-03, -7.2264024e-03],\n","        [-1.0133418e-03, -4.8498232e-03,  1.1487573e-03, ...,\n","         -6.5522511e-03, -8.9386530e-04, -8.7803602e-03],\n","        [ 1.1046773e-03, -5.4594758e-03,  3.9698211e-03, ...,\n","         -6.5345308e-03,  6.7721371e-04, -1.1056216e-02]],\n","\n","       [[ 9.0600859e-04, -1.5766339e-03,  6.9014344e-04, ...,\n","         -8.6845807e-04,  4.4788976e-04, -1.9482393e-03],\n","        [ 1.2163259e-03, -2.3775904e-03,  1.3042978e-03, ...,\n","         -1.5416453e-03, -7.3276438e-05, -3.0724988e-03],\n","        [ 3.1136634e-04, -2.2863299e-03,  2.1910297e-03, ...,\n","         -2.0098926e-03, -3.8952730e-05, -3.0664480e-03],\n","        ...,\n","        [ 7.0556398e-03,  1.9647405e-03, -1.7577214e-03, ...,\n","         -2.1459344e-03, -6.8160556e-03, -2.0643577e-03],\n","        [ 6.2483237e-03,  1.5224518e-03, -2.0823509e-03, ...,\n","         -2.6197899e-03, -7.8249900e-03, -1.8232983e-03],\n","        [ 5.7595856e-03,  1.8997533e-03, -1.3233164e-03, ...,\n","         -1.9007180e-03, -8.4501151e-03, -1.2721736e-03]]], dtype=float32)>"]},"metadata":{},"execution_count":28}],"metadata":{}},{"cell_type":"markdown","source":["<h2>Dense layer</h2>\n","We now create densely-connected neural network layer that would reshape the outputs tensor from  [30 x 20 x 128] to [30 x 20 x 10000].\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":29,"source":["dense = tf.keras.layers.Dense(vocab_size)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":30,"source":["logits_outputs  = dense(outputs)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":31,"source":["print(\"shape of the output from dense layer: \", logits_outputs.shape) #(batch_size, sequence_length, vocab_size)"],"outputs":[{"output_type":"stream","name":"stdout","text":["shape of the output from dense layer:  (30, 20, 10000)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["<h2>Activation layer</h2>\n","\n","A softmax activation layers is also then applied to derive the probability of the output being in any of the multiclass(10000 in this case) possibilities.\n"],"metadata":{}},{"cell_type":"code","execution_count":32,"source":["activation = tf.keras.layers.Activation('softmax')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":33,"source":["output_words_prob = activation(logits_outputs)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":34,"source":["print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)"],"outputs":[{"output_type":"stream","name":"stdout","text":["shape of the output from the activation layer:  (30, 20, 10000)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["Lets look at the probability of observing words for t=0 to t=20:\n"],"metadata":{}},{"cell_type":"code","execution_count":35,"source":["print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"],"outputs":[{"output_type":"stream","name":"stdout","text":["The probability of observing words in t=0 to t=20 tf.Tensor(\n","[[1.00005382e-04 9.99863405e-05 9.99944459e-05 ... 1.00006109e-04\n","  9.99925178e-05 1.00009645e-04]\n"," [1.00010446e-04 1.00001438e-04 9.99796612e-05 ... 1.00012672e-04\n","  9.99932890e-05 1.00012003e-04]\n"," [1.00002377e-04 1.00003359e-04 9.99725671e-05 ... 1.00021643e-04\n","  1.00000194e-04 1.00014659e-04]\n"," ...\n"," [1.00113240e-04 9.99521217e-05 1.00019453e-04 ... 1.00052413e-04\n","  9.99816257e-05 1.00109304e-04]\n"," [1.00097284e-04 9.99520344e-05 1.00011661e-04 ... 1.00046920e-04\n","  9.99585245e-05 1.00076722e-04]\n"," [1.00095393e-04 9.99554759e-05 1.00011064e-04 ... 1.00022931e-04\n","  9.99593467e-05 1.00053388e-04]], shape=(20, 10000), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["<h3>Prediction</h3>\n","What is the word correspond to the probability output? Lets use the maximum probability:\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":36,"source":["np.argmax(output_words_prob[0,0:num_steps], axis=1)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3522, 7502,   52,   52,   52, 2663, 6758, 6758, 6758, 6758, 4700,\n","       4700, 4700, 4206, 4206, 4206, 4206, 4206,  432,  432], dtype=int64)"]},"metadata":{},"execution_count":36}],"metadata":{}},{"cell_type":"markdown","source":["So, what is the ground truth for the first word of first sentence? You can get it from target tensor, if you want to find the embedding vector:\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":37,"source":["_targets[0]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n","       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996])"]},"metadata":{},"execution_count":37}],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<h4>Objective function</h4>\n","\n","How similar the predicted words are to the target words?\n","\n","Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n","\n","$$\\text{loss} = -\\frac{1}{N}\\sum\\_{i=1}^{N} \\ln p\\_{\\text{target}\\_i}$$\n","\n","This function is already implemented and available in TensorFlow through *tf.keras.losses.sparse_categorical_crossentropy*. It calculates the categorical cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.\n","\n","The arguments of this function are:\n","\n","<ul>\n","    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n","    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n","</ul>\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":38,"source":["def crossentropy(y_true, y_pred):\r\n","    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":39,"source":["loss  = crossentropy(_targets, output_words_prob)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["Lets look at the first 10 values of loss:\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":40,"source":["loss[0,:10]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=float32, numpy=\n","array([9.210689, 9.21067 , 9.210222, 9.210246, 9.210693, 9.209148,\n","       9.210111, 9.210266, 9.210664, 9.210809], dtype=float32)>"]},"metadata":{},"execution_count":40}],"metadata":{}},{"cell_type":"markdown","source":["Now, we define cost as average of the losses:\n"],"metadata":{}},{"cell_type":"code","execution_count":41,"source":["cost = tf.reduce_sum(loss / batch_size)\r\n","cost"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=184.20706>"]},"metadata":{},"execution_count":41}],"metadata":{}},{"cell_type":"markdown","source":["<h3>Training</h3>\n","\n","To do training for our network, we have to take the following steps:\n","\n","<ol>\n","    <li>Define the optimizer.</li>\n","    <li>Assemble layers to build model.</li>\n","    <li>Calculate the gradients based on the loss function.</li>\n","    <li>Apply the optimizer to the variables/gradients tuple.</li>\n","</ol>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<h4>1. Define Optimizer</h4>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":42,"source":["# Create a variable for the learning rate\r\n","lr = tf.Variable(0.0, trainable=False)\r\n","optimizer = tf.keras.optimizers.SGD(lr=lr, clipnorm=max_grad_norm)"],"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\yisiang\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  warnings.warn(\n"]}],"metadata":{}},{"cell_type":"markdown","source":["<h4>2. Assemble layers to build model.</h4>\n"],"metadata":{}},{"cell_type":"code","execution_count":43,"source":["model = tf.keras.Sequential()\r\n","model.add(embedding_layer)\r\n","model.add(layer)\r\n","model.add(dense)\r\n","model.add(activation)\r\n","model.compile(loss=crossentropy, optimizer=optimizer)\r\n","model.summary()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_vocab (Embedding)  (30, 20, 200)             2000000   \n","_________________________________________________________________\n","rnn (RNN)                    (30, 20, 128)             671088    \n","_________________________________________________________________\n","dense (Dense)                (30, 20, 10000)           1290000   \n","_________________________________________________________________\n","activation (Activation)      (30, 20, 10000)           0         \n","=================================================================\n","Total params: 3,961,088\n","Trainable params: 3,955,088\n","Non-trainable params: 6,000\n","_________________________________________________________________\n"]}],"metadata":{}},{"cell_type":"markdown","source":["<h4>2. Trainable Variables</h4>\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["Defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>.\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":44,"source":["# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\r\n","tvars = model.trainable_variables"],"outputs":[],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["Note: we can find the name and scope of all variables:\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":45,"source":["[v.name for v in tvars] "],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['embedding_vocab/embeddings:0',\n"," 'rnn/stacked_rnn_cells/lstm_cell/kernel:0',\n"," 'rnn/stacked_rnn_cells/lstm_cell/recurrent_kernel:0',\n"," 'rnn/stacked_rnn_cells/lstm_cell/bias:0',\n"," 'rnn/stacked_rnn_cells/lstm_cell_1/kernel:0',\n"," 'rnn/stacked_rnn_cells/lstm_cell_1/recurrent_kernel:0',\n"," 'rnn/stacked_rnn_cells/lstm_cell_1/bias:0',\n"," 'dense/kernel:0',\n"," 'dense/bias:0']"]},"metadata":{},"execution_count":45}],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<h4>3. Calculate the gradients based on the loss function</h4>\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["**Gradient**: The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by the <b>derivative</b> operation.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["First lets recall the gradient function using an toy example:\n","$$ z = \\left(2x^2 + 3xy\\right)$$\n"],"metadata":{}},{"cell_type":"code","execution_count":46,"source":["x = tf.constant(1.0)\r\n","y =  tf.constant(2.0)\r\n","with tf.GradientTape(persistent=True) as g:\r\n","    g.watch(x)\r\n","    g.watch(y)\r\n","    func_test = 2 * x * x + 3 * x * y"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["The <b>tf.gradients()</b> function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors—including variables. <b>tf.gradients(func, xs)</b> constructs symbolic partial derivatives of sum of <b>func</b> w\\.r.t. <i>x</i> in <b>xs</b>.\n","\n","Now, lets look at the derivitive w\\.r.t. <b>var_x</b>:\n","$$ \\frac{\\partial :}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 4x + 3y $$\n"],"metadata":{}},{"cell_type":"code","execution_count":47,"source":["var_grad = g.gradient(func_test, x) # Will compute to 10.0\r\n","print(var_grad)"],"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(10.0, shape=(), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["the derivative w\\.r.t. <b>var_y</b>:\n","$$ \\frac{\\partial :}{\\partial \\:y}\\left(2x^2 + 3xy\\right) = 3x $$\n"],"metadata":{}},{"cell_type":"code","execution_count":48,"source":["var_grad = g.gradient(func_test, y) # Will compute to 3.0\r\n","print(var_grad)"],"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n"]}],"metadata":{}},{"cell_type":"markdown","source":["Now, we can look at gradients w\\.r.t all variables:\n"],"metadata":{}},{"cell_type":"code","execution_count":49,"source":["with tf.GradientTape() as tape:\r\n","    # Forward pass.\r\n","    output_words_prob = model(_input_data)\r\n","    # Loss value for this batch.\r\n","    loss  = crossentropy(_targets, output_words_prob)\r\n","    cost = tf.reduce_sum(loss,axis=0) / batch_size"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":50,"source":["# Get gradients of loss wrt the trainable variables.\r\n","grad_t_list = tape.gradient(cost, tvars)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":51,"source":["print(grad_t_list)"],"outputs":[{"output_type":"stream","name":"stdout","text":["[<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x0000023F33D4EA30>, <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n","array([[ 9.8426582e-08, -2.6149706e-07, -1.3108722e-06, ...,\n","        -2.9901821e-07,  5.2866937e-08, -3.3483582e-07],\n","       [-8.0736311e-07, -1.7030023e-07,  2.0437244e-06, ...,\n","         2.1931214e-07, -2.2938838e-07, -6.6089811e-08],\n","       [-2.6701454e-07,  3.1854702e-07, -2.5876006e-06, ...,\n","         1.1360128e-07,  5.2582585e-08, -1.1820422e-07],\n","       ...,\n","       [ 6.5376042e-08,  1.1518611e-07,  2.8770842e-07, ...,\n","         5.1255893e-07,  1.4373623e-07, -1.8173776e-07],\n","       [-3.0296957e-07, -1.4937768e-08,  1.3339227e-06, ...,\n","        -5.0879358e-08, -5.5196935e-08,  4.3059771e-07],\n","       [-3.1670632e-07,  1.1931002e-07, -1.2902415e-06, ...,\n","         3.0312759e-07,  3.9359995e-07,  6.5888685e-08]], dtype=float32)>, <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n","array([[-1.4856409e-08,  8.4946635e-08,  2.6927866e-08, ...,\n","        -1.6469567e-08, -1.8119255e-08, -1.7986130e-07],\n","       [ 8.8127109e-08, -6.9109873e-09, -2.3669409e-07, ...,\n","        -1.2324097e-09, -3.8443165e-08, -1.5266005e-07],\n","       [ 3.2652306e-08, -7.7493027e-08,  2.2088932e-07, ...,\n","         2.8767786e-08,  3.5991988e-08,  4.0418414e-08],\n","       ...,\n","       [ 2.1155756e-08,  2.5989731e-08, -5.9537637e-08, ...,\n","         1.5835744e-07,  2.1186064e-08,  8.4119172e-08],\n","       [-2.5941659e-08,  6.9393536e-08, -1.4648990e-08, ...,\n","        -4.3546446e-08,  1.5603226e-07, -1.8199547e-07],\n","       [ 5.7780706e-08, -6.9412195e-08, -7.1408707e-08, ...,\n","        -7.7233722e-08, -8.1310304e-08, -3.8399540e-07]], dtype=float32)>, <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n","array([ 7.1588738e-06, -1.4935274e-05,  1.2910707e-05, ...,\n","        2.2493006e-05,  1.6727841e-05,  2.6543043e-05], dtype=float32)>, <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n","array([[-1.79179565e-07, -1.27508827e-07, -4.07169338e-08, ...,\n","        -2.77026345e-07, -2.45405545e-07, -2.16502336e-08],\n","       [ 3.13389776e-08, -3.54861925e-07,  4.38901431e-08, ...,\n","        -4.04397980e-07, -2.46214285e-08,  1.15783891e-07],\n","       [ 1.80567355e-07,  4.23208100e-08, -5.32491065e-08, ...,\n","         3.29901923e-07,  5.65133149e-08, -9.60323518e-08],\n","       ...,\n","       [-2.10949160e-08,  3.95489685e-08,  9.56026582e-08, ...,\n","         1.67426464e-07,  2.40934583e-07,  1.01444066e-07],\n","       [ 1.16225518e-08, -1.02172500e-07,  4.25942517e-08, ...,\n","         1.21656001e-08, -5.12427043e-08,  1.09385162e-07],\n","       [ 4.14128323e-08,  7.14397288e-08,  7.85347041e-08, ...,\n","         4.02350651e-08, -1.06525473e-08, -3.31479342e-08]], dtype=float32)>, <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n","array([[ 1.9166905e-07, -7.9276887e-08, -1.1471333e-07, ...,\n","        -5.3827311e-08,  1.1848523e-07, -1.2387873e-07],\n","       [ 7.8645684e-08,  1.5322163e-07, -4.3133429e-08, ...,\n","         9.2580699e-10, -8.3132150e-09, -1.7768979e-08],\n","       [ 5.5295217e-08, -3.8815635e-09, -2.0689988e-08, ...,\n","        -1.9599108e-07,  3.5266027e-08,  1.2403481e-07],\n","       ...,\n","       [ 8.5471825e-08,  1.3305802e-07,  9.3709929e-09, ...,\n","         3.8373179e-07,  6.6959721e-08, -2.4412057e-07],\n","       [ 8.3874873e-08,  4.7136460e-08,  3.5828478e-09, ...,\n","        -2.6400427e-08,  1.1030912e-07, -1.4979328e-08],\n","       [ 3.6601762e-09,  1.0898886e-07, -1.0891088e-07, ...,\n","         2.2755454e-07,  2.0332982e-07,  1.8904579e-08]], dtype=float32)>, <tf.Tensor: shape=(512,), dtype=float32, numpy=\n","array([ 1.25227980e-05,  6.57352211e-06,  1.72497639e-05, -2.77538711e-05,\n","        1.44017940e-05,  8.25638745e-06, -5.03256670e-05,  2.27165074e-05,\n","        4.15342984e-05,  3.22955520e-06, -1.90121154e-05,  3.29567047e-05,\n","        2.52463033e-05, -2.07783432e-05, -5.96906357e-05,  1.82367476e-05,\n","       -3.99478540e-06, -7.50372237e-07,  2.61816022e-05,  5.81094864e-05,\n","       -2.53958242e-05,  5.58006832e-05,  4.57684509e-05,  4.33963796e-05,\n","       -3.62544233e-05,  4.98306290e-05,  1.13175574e-05, -2.60802130e-06,\n","        8.50567358e-06, -3.86843058e-05,  2.22856979e-05,  4.75126899e-06,\n","       -1.24937924e-05,  1.88277227e-05,  1.60426898e-05, -1.31064808e-05,\n","        2.85324240e-05, -5.80020132e-05, -3.46304878e-05,  2.15423679e-05,\n","        7.12358378e-05,  1.23824993e-05, -6.73720351e-05,  2.26109696e-05,\n","       -2.56867406e-05, -1.10533392e-05, -5.43968326e-06, -4.47731691e-06,\n","        1.26009472e-05,  4.39342330e-06,  1.65537149e-05, -1.95429566e-05,\n","        2.03720228e-05, -1.89790389e-05,  3.78531258e-05,  2.41297021e-05,\n","       -8.16632964e-05, -1.53963119e-05, -1.64686571e-06,  3.04465793e-05,\n","       -2.14654410e-05,  1.81990872e-05,  4.64981204e-05, -2.48999077e-05,\n","       -8.71660359e-06, -2.82109631e-05,  1.40611091e-05,  1.09870698e-05,\n","        3.74702904e-05, -8.47599586e-06, -3.83667248e-05,  1.28825868e-05,\n","       -6.47197521e-06, -2.68616295e-05,  1.36439667e-06,  1.25068746e-05,\n","        1.88857775e-05, -1.80598818e-05,  3.73496223e-05, -3.54094518e-05,\n","       -8.70951189e-05,  5.40167057e-05, -7.56509980e-05, -2.74351914e-05,\n","        2.99440508e-05,  1.46283219e-05, -1.38508458e-05,  4.52238128e-05,\n","       -1.99711903e-05, -6.81198508e-05,  7.99001464e-06, -2.74232607e-05,\n","       -1.33169651e-05,  1.42981889e-05, -3.40002371e-05,  3.11031044e-06,\n","       -8.89772928e-05,  1.36932158e-07,  3.64074549e-05, -3.88198896e-05,\n","       -8.11883911e-06,  7.33575871e-06,  1.80980387e-05,  2.76938044e-06,\n","       -1.77768507e-05,  4.68219441e-05,  2.88908359e-05,  8.26115593e-06,\n","        9.09226401e-06, -1.26094128e-05,  6.10434154e-06,  2.35723892e-05,\n","       -7.43590590e-07, -8.87012357e-06, -3.93060145e-05, -1.15459816e-05,\n","        8.29417331e-05, -4.65338326e-06, -2.73723435e-06, -5.55530241e-05,\n","       -7.75770604e-06, -3.72013601e-05, -1.02066861e-05,  1.72640193e-06,\n","       -3.08648123e-05,  4.01579855e-05, -7.13372401e-06, -2.69712600e-05,\n","        1.71756838e-05,  1.77627171e-05,  1.32802325e-05, -2.85862261e-05,\n","        2.18659297e-05, -1.32074829e-05, -4.67812351e-05,  4.40559452e-05,\n","        7.48016755e-05,  4.86854151e-06, -4.95490531e-05,  5.32217819e-05,\n","        5.06869692e-05, -2.20357197e-05, -8.78828141e-05,  4.05160654e-06,\n","        6.44784814e-06, -7.96366294e-07,  2.74671984e-05,  8.90830852e-05,\n","       -1.80613970e-05,  8.46261537e-05,  5.05559547e-05,  2.25656077e-05,\n","       -9.34547788e-05,  5.64047004e-05,  3.60893027e-05,  2.01420007e-05,\n","       -1.63946661e-05, -3.66165332e-05,  5.57503608e-06,  1.02246331e-05,\n","       -2.44215389e-05,  2.37995373e-05,  2.58018772e-05, -5.53442987e-06,\n","        4.32720808e-05, -7.27820079e-05, -4.70054001e-05,  4.13231246e-05,\n","        1.00282472e-04,  1.77785732e-05, -9.70792607e-05,  5.78390027e-05,\n","       -5.48179887e-05, -4.54129113e-05, -4.80614653e-06,  4.17057890e-08,\n","        6.62214279e-06,  1.14444183e-05, -1.31743382e-05, -5.93465393e-06,\n","        3.65713386e-05, -1.99808983e-05,  3.41614468e-05,  3.17439262e-05,\n","       -1.29628199e-04, -2.88453521e-05,  2.98075793e-05,  1.79631643e-05,\n","       -1.56738497e-06,  1.79132512e-05,  5.19527166e-05, -2.58933142e-05,\n","       -1.07378237e-05, -2.66139177e-05,  1.10960955e-05,  3.84060013e-06,\n","        2.62060275e-05, -3.13423843e-05, -4.04433013e-05, -9.27981091e-06,\n","       -1.95328212e-05,  1.54909412e-06,  3.64537300e-06,  8.49736898e-06,\n","        1.60368054e-05, -2.95898317e-05,  4.54864457e-05, -4.53840257e-05,\n","       -1.33639813e-04,  7.05362982e-05, -8.32132500e-05, -2.97771894e-05,\n","        4.39632931e-05,  2.21424034e-05, -1.69899813e-05,  3.98752309e-05,\n","       -3.25575566e-05, -8.66150222e-05,  7.09793585e-06, -3.36451376e-05,\n","       -4.86099179e-06,  8.99405859e-06, -7.13394911e-05,  1.42353638e-05,\n","       -1.33257839e-04,  1.36297704e-05,  7.34369314e-05, -6.34514508e-05,\n","       -6.51259143e-07,  7.85272186e-06, -1.23128020e-05, -7.71597752e-06,\n","       -1.75061759e-05,  5.35305517e-05,  6.15002282e-05, -9.23524567e-07,\n","       -2.06893092e-05, -1.07913238e-05, -1.92103107e-05,  4.73414366e-06,\n","       -7.49958008e-06, -2.52551126e-05, -5.98222468e-05, -1.08539552e-05,\n","        9.33447009e-05,  1.74644338e-05,  2.34539766e-05, -5.85821472e-05,\n","       -1.77589063e-05, -5.48711032e-05,  2.28353679e-06,  2.61826062e-05,\n","       -4.21092263e-05,  5.15470310e-05,  8.27212079e-06, -2.23717489e-05,\n","        2.90888920e-02,  1.39701692e-02,  1.52981207e-02,  1.93453720e-03,\n","       -5.42774517e-03,  2.56844331e-03, -4.28391024e-02,  8.51206109e-03,\n","       -2.53348015e-02,  2.02937648e-02,  2.42603198e-02, -3.58732580e-03,\n","       -1.64818950e-03, -1.97712220e-02, -4.21366468e-02,  3.74501497e-02,\n","        3.72372940e-02,  1.24481448e-03, -1.39513034e-02, -4.72851321e-02,\n","        2.84845382e-02,  2.88173705e-02,  5.55205047e-02,  3.16720791e-02,\n","        2.06357911e-02, -1.95448082e-02,  1.07319439e-02,  1.50335757e-02,\n","       -5.44195026e-02,  7.27048051e-03,  3.18467021e-02,  1.14899455e-03,\n","        2.81542987e-02, -1.27006341e-02,  2.23245192e-02, -5.50939795e-03,\n","       -1.31003410e-02,  2.84768865e-02,  6.81590009e-03,  3.33742425e-02,\n","       -3.14431861e-02,  1.48733277e-02, -4.36236933e-02, -1.78567357e-02,\n","        4.80807479e-03,  1.52446954e-02,  7.31505686e-04,  1.85864186e-03,\n","        1.03958882e-04, -7.75342574e-04,  1.94816245e-03,  3.14499140e-02,\n","        2.67403834e-02,  1.16017675e-02, -6.82808645e-03, -4.31044102e-02,\n","        4.76861112e-02,  8.78597423e-03, -1.00691114e-02,  1.46654099e-02,\n","       -1.77127905e-02, -3.46296467e-03,  2.47455090e-02,  1.91755630e-02,\n","        3.27197313e-02, -1.12867672e-02, -2.29639150e-02, -4.42970991e-02,\n","       -3.59629914e-02,  1.67714879e-02,  6.66870084e-03,  8.15164670e-03,\n","       -4.46171407e-03, -2.05173641e-02, -1.43893436e-02, -6.34182012e-03,\n","       -2.27503665e-03, -7.10957684e-04,  8.60703457e-03,  3.33576053e-02,\n","        4.40766327e-02,  2.76877116e-02, -4.99490723e-02, -3.65024209e-02,\n","       -1.93111822e-02, -3.04040685e-02,  3.80656086e-02, -1.09737786e-03,\n","        1.80705469e-02, -3.81050855e-02, -9.67146363e-03, -3.26506793e-04,\n","        9.51658841e-03, -4.23550196e-02, -3.88932005e-02, -1.84129970e-03,\n","       -3.99975628e-02, -1.18675297e-02,  4.20128703e-02,  3.11283041e-02,\n","       -4.83601168e-03, -1.21664293e-02, -2.12737545e-03, -3.19858305e-02,\n","        7.16994517e-04,  1.03815608e-02, -2.41325349e-02,  3.55214067e-02,\n","        2.01395191e-02,  2.73850118e-03,  3.09120454e-02,  3.37748900e-02,\n","        1.90726742e-02, -4.14992049e-02, -1.72700789e-02, -8.52045789e-03,\n","       -5.26538715e-02, -2.85984818e-02, -4.44505655e-04, -3.33192684e-02,\n","       -4.78443503e-03, -2.08135154e-02, -5.27998060e-03,  1.75536219e-02,\n","       -2.52649784e-02,  2.04230882e-02,  1.86411152e-03, -5.94112976e-03,\n","        1.81231044e-05,  6.40016697e-06,  2.75242619e-05, -2.30054047e-05,\n","        1.40301481e-05,  5.90365926e-06, -5.91347343e-05,  2.83413046e-05,\n","        4.75625275e-05,  1.43651528e-06, -2.73276673e-05,  3.89184388e-05,\n","        2.11973202e-05, -2.04691714e-05, -7.29079693e-05,  1.08381191e-05,\n","       -9.07536923e-06,  8.59156444e-07,  2.12517298e-05,  6.53975003e-05,\n","       -2.71324934e-05,  6.10096213e-05,  5.28365454e-05,  5.92816723e-05,\n","       -5.57748026e-05,  5.37205597e-05,  2.04566713e-05, -2.33081300e-06,\n","        1.29610953e-05, -3.77350079e-05,  1.78168029e-05,  3.46955767e-06,\n","       -2.29912512e-05,  1.90375886e-05,  4.51292908e-06, -2.04697280e-05,\n","        2.16188018e-05, -6.22227526e-05, -4.14939823e-05,  2.45203846e-05,\n","        7.36763250e-05,  1.03402581e-05, -8.25178140e-05,  2.99416970e-05,\n","       -2.84617945e-05, -1.86528487e-05, -3.90700188e-06,  6.84727092e-06,\n","        1.26604555e-05,  1.18354365e-07,  2.02676492e-05, -2.07683206e-05,\n","        1.49359666e-05, -1.14941904e-05,  3.49424845e-05,  2.72732832e-05,\n","       -8.74094185e-05, -1.74500965e-05,  2.71863792e-06,  3.58954858e-05,\n","       -3.22070518e-05,  1.50764718e-05,  5.43384158e-05, -2.19105023e-05,\n","       -1.06503876e-05, -3.23486493e-05,  1.19151609e-05,  8.81807046e-06,\n","        4.11193178e-05, -1.50383021e-05, -3.16105361e-05,  1.09989751e-05,\n","       -3.16488968e-06, -2.75659440e-05, -3.10715950e-06,  1.39735339e-05,\n","        1.41147211e-05, -1.74301676e-05,  3.30113216e-05, -3.59682963e-05,\n","       -9.76821029e-05,  6.49802314e-05, -7.92585925e-05, -3.35777695e-05,\n","        4.14912211e-05,  2.87488347e-05, -1.70220592e-05,  4.43562749e-05,\n","       -2.57967331e-05, -7.92528881e-05,  1.76636786e-05, -3.22734268e-05,\n","       -2.13898093e-05,  1.21022704e-05, -4.35235670e-05,  6.40963708e-06,\n","       -9.52901246e-05,  4.43373392e-06,  4.01683938e-05, -4.14642054e-05,\n","       -1.23275559e-05,  1.29488581e-06,  1.69134873e-05, -4.26720226e-06,\n","       -1.25309334e-05,  4.59568037e-05,  3.25053188e-05,  5.33972889e-06,\n","        8.50897050e-06, -1.26109262e-05,  1.84122091e-05,  1.97945574e-05,\n","       -3.15172701e-06, -6.79694494e-06, -3.92108814e-05, -5.92630931e-06,\n","        8.60806176e-05,  2.13761086e-06, -1.24741900e-05, -5.13924533e-05,\n","       -5.04068339e-06, -4.75760498e-05, -8.65103630e-06, -4.21736149e-06,\n","       -3.45193548e-05,  4.62854368e-05,  1.00740726e-06, -2.58780237e-05],\n","      dtype=float32)>, <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n","array([[-5.9384602e-04, -1.2076248e-03, -1.6505856e-04, ...,\n","         1.2172277e-06,  1.2181896e-06,  1.2179332e-06],\n","       [ 8.7358349e-04,  3.4240971e-04, -7.3010905e-04, ...,\n","        -5.4065111e-07, -5.3965090e-07, -5.4141208e-07],\n","       [ 8.5077045e-04, -6.7667553e-04,  1.4478380e-03, ...,\n","        -1.7542898e-06, -1.7523198e-06, -1.7521135e-06],\n","       ...,\n","       [-4.1538430e-04, -9.5038791e-04, -2.4819651e-03, ...,\n","         3.6643651e-06,  3.6658737e-06,  3.6657909e-06],\n","       [ 1.0295262e-03, -9.7529351e-04, -2.3906739e-04, ...,\n","        -1.1697032e-06, -1.1698928e-06, -1.1702826e-06],\n","       [ 7.3416501e-05,  1.1746160e-05, -5.3006317e-04, ...,\n","         3.0896655e-07,  3.1313806e-07,  3.1227987e-07]], dtype=float32)>, <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n","array([-0.7979979 , -1.0313317 , -1.0313313 , ...,  0.00199996,\n","        0.00200009,  0.00199999], dtype=float32)>]\n"]}],"metadata":{}},{"cell_type":"markdown","source":["now, we have a list of tensors, t-list. We can use it to find clipped tensors. <b>clip_by_global_norm</b> clips values of multiple tensors by the ratio of the sum of their norms.\n","\n","<b>clip_by_global_norm</b> get <i>t-list</i> as input and returns 2 things:\n","\n","<ul>\n","    <li>a list of clipped tensors, so called <i>list_clipped</i></li> \n","    <li>the global norm (global_norm) of all tensors in t_list</li> \n","</ul>\n"],"metadata":{"button":false,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":52,"source":["# Define the gradient clipping threshold\r\n","grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\r\n","grads"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x23f31d7b730>,\n"," <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n"," array([[ 9.8426582e-08, -2.6149706e-07, -1.3108722e-06, ...,\n","         -2.9901821e-07,  5.2866937e-08, -3.3483582e-07],\n","        [-8.0736311e-07, -1.7030023e-07,  2.0437244e-06, ...,\n","          2.1931214e-07, -2.2938838e-07, -6.6089811e-08],\n","        [-2.6701454e-07,  3.1854702e-07, -2.5876006e-06, ...,\n","          1.1360128e-07,  5.2582585e-08, -1.1820422e-07],\n","        ...,\n","        [ 6.5376042e-08,  1.1518611e-07,  2.8770842e-07, ...,\n","          5.1255893e-07,  1.4373623e-07, -1.8173776e-07],\n","        [-3.0296957e-07, -1.4937768e-08,  1.3339227e-06, ...,\n","         -5.0879358e-08, -5.5196935e-08,  4.3059771e-07],\n","        [-3.1670632e-07,  1.1931002e-07, -1.2902415e-06, ...,\n","          3.0312759e-07,  3.9359995e-07,  6.5888685e-08]], dtype=float32)>,\n"," <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n"," array([[-1.4856409e-08,  8.4946635e-08,  2.6927866e-08, ...,\n","         -1.6469567e-08, -1.8119255e-08, -1.7986130e-07],\n","        [ 8.8127109e-08, -6.9109873e-09, -2.3669409e-07, ...,\n","         -1.2324097e-09, -3.8443165e-08, -1.5266005e-07],\n","        [ 3.2652306e-08, -7.7493027e-08,  2.2088932e-07, ...,\n","          2.8767786e-08,  3.5991988e-08,  4.0418414e-08],\n","        ...,\n","        [ 2.1155756e-08,  2.5989731e-08, -5.9537637e-08, ...,\n","          1.5835744e-07,  2.1186064e-08,  8.4119172e-08],\n","        [-2.5941659e-08,  6.9393536e-08, -1.4648990e-08, ...,\n","         -4.3546446e-08,  1.5603226e-07, -1.8199547e-07],\n","        [ 5.7780706e-08, -6.9412195e-08, -7.1408707e-08, ...,\n","         -7.7233722e-08, -8.1310304e-08, -3.8399540e-07]], dtype=float32)>,\n"," <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n"," array([ 7.1588738e-06, -1.4935274e-05,  1.2910707e-05, ...,\n","         2.2493006e-05,  1.6727841e-05,  2.6543043e-05], dtype=float32)>,\n"," <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n"," array([[-1.79179565e-07, -1.27508827e-07, -4.07169338e-08, ...,\n","         -2.77026345e-07, -2.45405545e-07, -2.16502336e-08],\n","        [ 3.13389776e-08, -3.54861925e-07,  4.38901431e-08, ...,\n","         -4.04397980e-07, -2.46214285e-08,  1.15783891e-07],\n","        [ 1.80567355e-07,  4.23208100e-08, -5.32491065e-08, ...,\n","          3.29901923e-07,  5.65133149e-08, -9.60323518e-08],\n","        ...,\n","        [-2.10949160e-08,  3.95489685e-08,  9.56026582e-08, ...,\n","          1.67426464e-07,  2.40934583e-07,  1.01444066e-07],\n","        [ 1.16225518e-08, -1.02172500e-07,  4.25942517e-08, ...,\n","          1.21656001e-08, -5.12427043e-08,  1.09385162e-07],\n","        [ 4.14128323e-08,  7.14397288e-08,  7.85347041e-08, ...,\n","          4.02350651e-08, -1.06525473e-08, -3.31479342e-08]], dtype=float32)>,\n"," <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n"," array([[ 1.9166905e-07, -7.9276887e-08, -1.1471333e-07, ...,\n","         -5.3827311e-08,  1.1848523e-07, -1.2387873e-07],\n","        [ 7.8645684e-08,  1.5322163e-07, -4.3133429e-08, ...,\n","          9.2580699e-10, -8.3132150e-09, -1.7768979e-08],\n","        [ 5.5295217e-08, -3.8815635e-09, -2.0689988e-08, ...,\n","         -1.9599108e-07,  3.5266027e-08,  1.2403481e-07],\n","        ...,\n","        [ 8.5471825e-08,  1.3305802e-07,  9.3709929e-09, ...,\n","          3.8373179e-07,  6.6959721e-08, -2.4412057e-07],\n","        [ 8.3874873e-08,  4.7136460e-08,  3.5828478e-09, ...,\n","         -2.6400427e-08,  1.1030912e-07, -1.4979328e-08],\n","        [ 3.6601762e-09,  1.0898886e-07, -1.0891088e-07, ...,\n","          2.2755454e-07,  2.0332982e-07,  1.8904579e-08]], dtype=float32)>,\n"," <tf.Tensor: shape=(512,), dtype=float32, numpy=\n"," array([ 1.25227980e-05,  6.57352211e-06,  1.72497639e-05, -2.77538711e-05,\n","         1.44017940e-05,  8.25638745e-06, -5.03256670e-05,  2.27165074e-05,\n","         4.15342984e-05,  3.22955520e-06, -1.90121154e-05,  3.29567047e-05,\n","         2.52463033e-05, -2.07783432e-05, -5.96906357e-05,  1.82367476e-05,\n","        -3.99478540e-06, -7.50372237e-07,  2.61816022e-05,  5.81094864e-05,\n","        -2.53958242e-05,  5.58006832e-05,  4.57684509e-05,  4.33963796e-05,\n","        -3.62544233e-05,  4.98306290e-05,  1.13175574e-05, -2.60802130e-06,\n","         8.50567358e-06, -3.86843058e-05,  2.22856979e-05,  4.75126899e-06,\n","        -1.24937924e-05,  1.88277227e-05,  1.60426898e-05, -1.31064808e-05,\n","         2.85324240e-05, -5.80020132e-05, -3.46304878e-05,  2.15423679e-05,\n","         7.12358378e-05,  1.23824993e-05, -6.73720351e-05,  2.26109696e-05,\n","        -2.56867406e-05, -1.10533392e-05, -5.43968326e-06, -4.47731691e-06,\n","         1.26009472e-05,  4.39342330e-06,  1.65537149e-05, -1.95429566e-05,\n","         2.03720228e-05, -1.89790389e-05,  3.78531258e-05,  2.41297021e-05,\n","        -8.16632964e-05, -1.53963119e-05, -1.64686571e-06,  3.04465793e-05,\n","        -2.14654410e-05,  1.81990872e-05,  4.64981204e-05, -2.48999077e-05,\n","        -8.71660359e-06, -2.82109631e-05,  1.40611091e-05,  1.09870698e-05,\n","         3.74702904e-05, -8.47599586e-06, -3.83667248e-05,  1.28825868e-05,\n","        -6.47197521e-06, -2.68616295e-05,  1.36439667e-06,  1.25068746e-05,\n","         1.88857775e-05, -1.80598818e-05,  3.73496223e-05, -3.54094518e-05,\n","        -8.70951189e-05,  5.40167057e-05, -7.56509980e-05, -2.74351914e-05,\n","         2.99440508e-05,  1.46283219e-05, -1.38508458e-05,  4.52238128e-05,\n","        -1.99711903e-05, -6.81198508e-05,  7.99001464e-06, -2.74232607e-05,\n","        -1.33169651e-05,  1.42981889e-05, -3.40002371e-05,  3.11031044e-06,\n","        -8.89772928e-05,  1.36932158e-07,  3.64074549e-05, -3.88198896e-05,\n","        -8.11883911e-06,  7.33575871e-06,  1.80980387e-05,  2.76938044e-06,\n","        -1.77768507e-05,  4.68219441e-05,  2.88908359e-05,  8.26115593e-06,\n","         9.09226401e-06, -1.26094128e-05,  6.10434154e-06,  2.35723892e-05,\n","        -7.43590590e-07, -8.87012357e-06, -3.93060145e-05, -1.15459816e-05,\n","         8.29417331e-05, -4.65338326e-06, -2.73723435e-06, -5.55530241e-05,\n","        -7.75770604e-06, -3.72013601e-05, -1.02066861e-05,  1.72640193e-06,\n","        -3.08648123e-05,  4.01579855e-05, -7.13372401e-06, -2.69712600e-05,\n","         1.71756838e-05,  1.77627171e-05,  1.32802325e-05, -2.85862261e-05,\n","         2.18659297e-05, -1.32074829e-05, -4.67812351e-05,  4.40559452e-05,\n","         7.48016755e-05,  4.86854151e-06, -4.95490531e-05,  5.32217819e-05,\n","         5.06869692e-05, -2.20357197e-05, -8.78828141e-05,  4.05160654e-06,\n","         6.44784814e-06, -7.96366294e-07,  2.74671984e-05,  8.90830852e-05,\n","        -1.80613970e-05,  8.46261537e-05,  5.05559547e-05,  2.25656077e-05,\n","        -9.34547788e-05,  5.64047004e-05,  3.60893027e-05,  2.01420007e-05,\n","        -1.63946661e-05, -3.66165332e-05,  5.57503608e-06,  1.02246331e-05,\n","        -2.44215389e-05,  2.37995373e-05,  2.58018772e-05, -5.53442987e-06,\n","         4.32720808e-05, -7.27820079e-05, -4.70054001e-05,  4.13231246e-05,\n","         1.00282472e-04,  1.77785732e-05, -9.70792607e-05,  5.78390027e-05,\n","        -5.48179887e-05, -4.54129113e-05, -4.80614653e-06,  4.17057890e-08,\n","         6.62214279e-06,  1.14444183e-05, -1.31743382e-05, -5.93465393e-06,\n","         3.65713386e-05, -1.99808983e-05,  3.41614468e-05,  3.17439262e-05,\n","        -1.29628199e-04, -2.88453521e-05,  2.98075793e-05,  1.79631643e-05,\n","        -1.56738497e-06,  1.79132512e-05,  5.19527166e-05, -2.58933142e-05,\n","        -1.07378237e-05, -2.66139177e-05,  1.10960955e-05,  3.84060013e-06,\n","         2.62060275e-05, -3.13423843e-05, -4.04433013e-05, -9.27981091e-06,\n","        -1.95328212e-05,  1.54909412e-06,  3.64537300e-06,  8.49736898e-06,\n","         1.60368054e-05, -2.95898317e-05,  4.54864457e-05, -4.53840257e-05,\n","        -1.33639813e-04,  7.05362982e-05, -8.32132500e-05, -2.97771894e-05,\n","         4.39632931e-05,  2.21424034e-05, -1.69899813e-05,  3.98752309e-05,\n","        -3.25575566e-05, -8.66150222e-05,  7.09793585e-06, -3.36451376e-05,\n","        -4.86099179e-06,  8.99405859e-06, -7.13394911e-05,  1.42353638e-05,\n","        -1.33257839e-04,  1.36297704e-05,  7.34369314e-05, -6.34514508e-05,\n","        -6.51259143e-07,  7.85272186e-06, -1.23128020e-05, -7.71597752e-06,\n","        -1.75061759e-05,  5.35305517e-05,  6.15002282e-05, -9.23524567e-07,\n","        -2.06893092e-05, -1.07913238e-05, -1.92103107e-05,  4.73414366e-06,\n","        -7.49958008e-06, -2.52551126e-05, -5.98222468e-05, -1.08539552e-05,\n","         9.33447009e-05,  1.74644338e-05,  2.34539766e-05, -5.85821472e-05,\n","        -1.77589063e-05, -5.48711032e-05,  2.28353679e-06,  2.61826062e-05,\n","        -4.21092263e-05,  5.15470310e-05,  8.27212079e-06, -2.23717489e-05,\n","         2.90888920e-02,  1.39701692e-02,  1.52981207e-02,  1.93453720e-03,\n","        -5.42774517e-03,  2.56844331e-03, -4.28391024e-02,  8.51206109e-03,\n","        -2.53348015e-02,  2.02937648e-02,  2.42603198e-02, -3.58732580e-03,\n","        -1.64818950e-03, -1.97712220e-02, -4.21366468e-02,  3.74501497e-02,\n","         3.72372940e-02,  1.24481448e-03, -1.39513034e-02, -4.72851321e-02,\n","         2.84845382e-02,  2.88173705e-02,  5.55205047e-02,  3.16720791e-02,\n","         2.06357911e-02, -1.95448082e-02,  1.07319439e-02,  1.50335757e-02,\n","        -5.44195026e-02,  7.27048051e-03,  3.18467021e-02,  1.14899455e-03,\n","         2.81542987e-02, -1.27006341e-02,  2.23245192e-02, -5.50939795e-03,\n","        -1.31003410e-02,  2.84768865e-02,  6.81590009e-03,  3.33742425e-02,\n","        -3.14431861e-02,  1.48733277e-02, -4.36236933e-02, -1.78567357e-02,\n","         4.80807479e-03,  1.52446954e-02,  7.31505686e-04,  1.85864186e-03,\n","         1.03958882e-04, -7.75342574e-04,  1.94816245e-03,  3.14499140e-02,\n","         2.67403834e-02,  1.16017675e-02, -6.82808645e-03, -4.31044102e-02,\n","         4.76861112e-02,  8.78597423e-03, -1.00691114e-02,  1.46654099e-02,\n","        -1.77127905e-02, -3.46296467e-03,  2.47455090e-02,  1.91755630e-02,\n","         3.27197313e-02, -1.12867672e-02, -2.29639150e-02, -4.42970991e-02,\n","        -3.59629914e-02,  1.67714879e-02,  6.66870084e-03,  8.15164670e-03,\n","        -4.46171407e-03, -2.05173641e-02, -1.43893436e-02, -6.34182012e-03,\n","        -2.27503665e-03, -7.10957684e-04,  8.60703457e-03,  3.33576053e-02,\n","         4.40766327e-02,  2.76877116e-02, -4.99490723e-02, -3.65024209e-02,\n","        -1.93111822e-02, -3.04040685e-02,  3.80656086e-02, -1.09737786e-03,\n","         1.80705469e-02, -3.81050855e-02, -9.67146363e-03, -3.26506793e-04,\n","         9.51658841e-03, -4.23550196e-02, -3.88932005e-02, -1.84129970e-03,\n","        -3.99975628e-02, -1.18675297e-02,  4.20128703e-02,  3.11283041e-02,\n","        -4.83601168e-03, -1.21664293e-02, -2.12737545e-03, -3.19858305e-02,\n","         7.16994517e-04,  1.03815608e-02, -2.41325349e-02,  3.55214067e-02,\n","         2.01395191e-02,  2.73850118e-03,  3.09120454e-02,  3.37748900e-02,\n","         1.90726742e-02, -4.14992049e-02, -1.72700789e-02, -8.52045789e-03,\n","        -5.26538715e-02, -2.85984818e-02, -4.44505655e-04, -3.33192684e-02,\n","        -4.78443503e-03, -2.08135154e-02, -5.27998060e-03,  1.75536219e-02,\n","        -2.52649784e-02,  2.04230882e-02,  1.86411152e-03, -5.94112976e-03,\n","         1.81231044e-05,  6.40016697e-06,  2.75242619e-05, -2.30054047e-05,\n","         1.40301481e-05,  5.90365926e-06, -5.91347343e-05,  2.83413046e-05,\n","         4.75625275e-05,  1.43651528e-06, -2.73276673e-05,  3.89184388e-05,\n","         2.11973202e-05, -2.04691714e-05, -7.29079693e-05,  1.08381191e-05,\n","        -9.07536923e-06,  8.59156444e-07,  2.12517298e-05,  6.53975003e-05,\n","        -2.71324934e-05,  6.10096213e-05,  5.28365454e-05,  5.92816723e-05,\n","        -5.57748026e-05,  5.37205597e-05,  2.04566713e-05, -2.33081300e-06,\n","         1.29610953e-05, -3.77350079e-05,  1.78168029e-05,  3.46955767e-06,\n","        -2.29912512e-05,  1.90375886e-05,  4.51292908e-06, -2.04697280e-05,\n","         2.16188018e-05, -6.22227526e-05, -4.14939823e-05,  2.45203846e-05,\n","         7.36763250e-05,  1.03402581e-05, -8.25178140e-05,  2.99416970e-05,\n","        -2.84617945e-05, -1.86528487e-05, -3.90700188e-06,  6.84727092e-06,\n","         1.26604555e-05,  1.18354365e-07,  2.02676492e-05, -2.07683206e-05,\n","         1.49359666e-05, -1.14941904e-05,  3.49424845e-05,  2.72732832e-05,\n","        -8.74094185e-05, -1.74500965e-05,  2.71863792e-06,  3.58954858e-05,\n","        -3.22070518e-05,  1.50764718e-05,  5.43384158e-05, -2.19105023e-05,\n","        -1.06503876e-05, -3.23486493e-05,  1.19151609e-05,  8.81807046e-06,\n","         4.11193178e-05, -1.50383021e-05, -3.16105361e-05,  1.09989751e-05,\n","        -3.16488968e-06, -2.75659440e-05, -3.10715950e-06,  1.39735339e-05,\n","         1.41147211e-05, -1.74301676e-05,  3.30113216e-05, -3.59682963e-05,\n","        -9.76821029e-05,  6.49802314e-05, -7.92585925e-05, -3.35777695e-05,\n","         4.14912211e-05,  2.87488347e-05, -1.70220592e-05,  4.43562749e-05,\n","        -2.57967331e-05, -7.92528881e-05,  1.76636786e-05, -3.22734268e-05,\n","        -2.13898093e-05,  1.21022704e-05, -4.35235670e-05,  6.40963708e-06,\n","        -9.52901246e-05,  4.43373392e-06,  4.01683938e-05, -4.14642054e-05,\n","        -1.23275559e-05,  1.29488581e-06,  1.69134873e-05, -4.26720226e-06,\n","        -1.25309334e-05,  4.59568037e-05,  3.25053188e-05,  5.33972889e-06,\n","         8.50897050e-06, -1.26109262e-05,  1.84122091e-05,  1.97945574e-05,\n","        -3.15172701e-06, -6.79694494e-06, -3.92108814e-05, -5.92630931e-06,\n","         8.60806176e-05,  2.13761086e-06, -1.24741900e-05, -5.13924533e-05,\n","        -5.04068339e-06, -4.75760498e-05, -8.65103630e-06, -4.21736149e-06,\n","        -3.45193548e-05,  4.62854368e-05,  1.00740726e-06, -2.58780237e-05],\n","       dtype=float32)>,\n"," <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n"," array([[-5.9384602e-04, -1.2076248e-03, -1.6505856e-04, ...,\n","          1.2172277e-06,  1.2181896e-06,  1.2179332e-06],\n","        [ 8.7358349e-04,  3.4240971e-04, -7.3010905e-04, ...,\n","         -5.4065111e-07, -5.3965090e-07, -5.4141208e-07],\n","        [ 8.5077045e-04, -6.7667553e-04,  1.4478380e-03, ...,\n","         -1.7542898e-06, -1.7523198e-06, -1.7521135e-06],\n","        ...,\n","        [-4.1538430e-04, -9.5038791e-04, -2.4819651e-03, ...,\n","          3.6643651e-06,  3.6658737e-06,  3.6657909e-06],\n","        [ 1.0295262e-03, -9.7529351e-04, -2.3906739e-04, ...,\n","         -1.1697032e-06, -1.1698928e-06, -1.1702826e-06],\n","        [ 7.3416501e-05,  1.1746160e-05, -5.3006317e-04, ...,\n","          3.0896655e-07,  3.1313806e-07,  3.1227987e-07]], dtype=float32)>,\n"," <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n"," array([-0.7979979 , -1.0313317 , -1.0313313 , ...,  0.00199996,\n","         0.00200009,  0.00199999], dtype=float32)>]"]},"metadata":{},"execution_count":52}],"metadata":{}},{"cell_type":"markdown","source":["<h4> 4.Apply the optimizer to the variables/gradients tuple. </h4>\n"],"metadata":{}},{"cell_type":"code","execution_count":53,"source":["# Create the training TensorFlow Operation through our optimizer\r\n","train_op = optimizer.apply_gradients(zip(grads, tvars))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":["<a id=\"ltsm\"></a>\n","\n","<h2>LSTM</h2>\n"],"metadata":{}},{"cell_type":"markdown","source":["We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:\n","\n","<ul>\n","    <li>We have to create the model in accordance with our defined hyperparameters</li>\n","    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n","    <li>We have to create the word embeddings and point them to the input data</li>\n","    <li>We have to create the input structure for our RNN</li>\n","    <li>We need to create a logistic structure to return the probability of our words</li>\n","    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n","    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n","</ul>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":54,"source":["class PTBModel(object):\r\n","\r\n","\r\n","    def __init__(self):\r\n","        ######################################\r\n","        # Setting parameters for ease of use #\r\n","        ######################################\r\n","        self.batch_size = batch_size\r\n","        self.num_steps = num_steps\r\n","        self.hidden_size_l1 = hidden_size_l1\r\n","        self.hidden_size_l2 = hidden_size_l2\r\n","        self.vocab_size = vocab_size\r\n","        self.embeding_vector_size = embeding_vector_size\r\n","        # Create a variable for the learning rate\r\n","        self._lr = 1.0\r\n","        \r\n","        ###############################################################################\r\n","        # Initializing the model using keras Sequential API  #\r\n","        ###############################################################################\r\n","        \r\n","        self._model = tf.keras.models.Sequential()\r\n","        \r\n","        ####################################################################\r\n","        # Creating the word embeddings layer and adding it to the sequence #\r\n","        ####################################################################\r\n","        with tf.device(\"/cpu:0\"):\r\n","            # Create the embeddings for our input data. Size is hidden size.\r\n","            self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\r\n","            self._model.add(self._embedding_layer)\r\n","            \r\n","\r\n","        ##########################################################################\r\n","        # Creating the LSTM cell structure and connect it with the RNN structure #\r\n","        ##########################################################################\r\n","        # Create the LSTM Cells. \r\n","        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\r\n","        # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A). \r\n","        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\r\n","        lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\r\n","        lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\r\n","        \r\n","\r\n","        \r\n","        # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\r\n","        # RNN cell composed sequentially of stacked simple cells.\r\n","        stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\r\n","\r\n","\r\n","        \r\n","\r\n","        ############################################\r\n","        # Creating the input structure for our RNN #\r\n","        ############################################\r\n","        # Input structure is 20x[30x200]\r\n","        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\r\n","        # The input structure is fed from the embeddings, which are filled in by the input data\r\n","        # Feeding a batch of b sentences to a RNN:\r\n","        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \r\n","        # In step 2,  second word of each of the b sentences is input in parallel. \r\n","        # The parallelism is only for efficiency.  \r\n","        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \r\n","        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \r\n","\r\n","        ########################################################################################################\r\n","        # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\r\n","        ########################################################################################################\r\n","        \r\n","        self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\r\n","        \r\n","        # Define the initial state, i.e., the model state for the very first data point\r\n","        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\r\n","        self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\r\n","        self._RNNlayer.inital_state = self._initial_state\r\n","    \r\n","        ############################################\r\n","        # Adding RNN layer to keras sequential API #\r\n","        ############################################        \r\n","        self._model.add(self._RNNlayer)\r\n","        \r\n","        #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\r\n","        #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\r\n","        \r\n","        \r\n","        ####################################################################################################\r\n","        # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\r\n","        ####################################################################################################\r\n","        self._dense = tf.keras.layers.Dense(self.vocab_size)\r\n","        self._model.add(self._dense)\r\n"," \r\n","        \r\n","        ####################################################################################################\r\n","        # Adding softmax activation layer and deriving probability to each class and adding layer to model #\r\n","        ####################################################################################################\r\n","        self._activation = tf.keras.layers.Activation('softmax')\r\n","        self._model.add(self._activation)\r\n","\r\n","        ##########################################################\r\n","        # Instantiating the stochastic gradient decent optimizer #\r\n","        ########################################################## \r\n","        self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\r\n","        \r\n","        \r\n","        ##############################################################################\r\n","        # Compiling and summarizing the model stacked using the keras sequential API #\r\n","        ##############################################################################\r\n","        self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\r\n","        self._model.summary()\r\n","\r\n","\r\n","    def crossentropy(self,y_true, y_pred):\r\n","        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\r\n","\r\n","    def train_batch(self,_input_data,_targets):\r\n","        #################################################\r\n","        # Creating the Training Operation for our Model #\r\n","        #################################################\r\n","        # Create a variable for the learning rate\r\n","        self._lr = tf.Variable(0.0, trainable=False)\r\n","        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\r\n","        tvars = self._model.trainable_variables\r\n","        # Define the gradient clipping threshold\r\n","        with tf.GradientTape() as tape:\r\n","            # Forward pass.\r\n","            output_words_prob = self._model(_input_data)\r\n","            # Loss value for this batch.\r\n","            loss  = self.crossentropy(_targets, output_words_prob)\r\n","            # average across batch and reduce sum\r\n","            cost = tf.reduce_sum(loss/ self.batch_size)\r\n","        # Get gradients of loss wrt the trainable variables.\r\n","        grad_t_list = tape.gradient(cost, tvars)\r\n","        # Define the gradient clipping threshold\r\n","        grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\r\n","        # Create the training TensorFlow Operation through our optimizer\r\n","        train_op = self._optimizer.apply_gradients(zip(grads, tvars))\r\n","        return cost\r\n","        \r\n","    def test_batch(self,_input_data,_targets):\r\n","        #################################################\r\n","        # Creating the Testing Operation for our Model #\r\n","        #################################################\r\n","        output_words_prob = self._model(_input_data)\r\n","        loss  = self.crossentropy(_targets, output_words_prob)\r\n","        # average across batch and reduce sum\r\n","        cost = tf.reduce_sum(loss/ self.batch_size)\r\n","\r\n","        return cost\r\n","    @classmethod\r\n","    def instance(cls) : \r\n","        return PTBModel()"],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"tags":[]}},{"cell_type":"markdown","source":["With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n","\n","What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":55,"source":["\r\n","########################################################################################################################\r\n","# run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\r\n","########################################################################################################################\r\n","def run_one_epoch(m, data,is_training=True,verbose=False):\r\n","\r\n","    #Define the epoch size based on the length of the data, batch size and the number of steps\r\n","    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\r\n","    start_time = time.time()\r\n","    costs = 0.\r\n","    iters = 0\r\n","    \r\n","    m._model.reset_states()\r\n","    \r\n","    #For each step and data point\r\n","    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\r\n","        \r\n","        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\r\n","        #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\r\n","        if is_training : \r\n","            loss=  m.train_batch(x, y)\r\n","        else :\r\n","            loss = m.test_batch(x, y)\r\n","                                   \r\n","\r\n","        #Add returned cost to costs (which keeps track of the total costs for this epoch)\r\n","        costs += loss\r\n","        \r\n","        #Add number of steps to iteration counter\r\n","        iters += m.num_steps\r\n","\r\n","        if verbose and step % (epoch_size // 10) == 10:\r\n","            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\r\n","        \r\n","\r\n","\r\n","    # Returns the Perplexity rating for us to keep track of how the model is evolving\r\n","    return np.exp(costs / iters)\r\n"],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["Now, we create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, and then trains and evaluates the model on both a testing and a validating subset of data.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"code","execution_count":56,"source":["# Reads the data and separates it into training data, validation data and testing data\r\n","raw_data = reader.ptb_raw_data(data_dir)\r\n","train_data, valid_data, test_data, _, _ = raw_data"],"outputs":[],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"tags":[]}},{"cell_type":"code","execution_count":57,"source":["# Instantiates the PTBModel class\r\n","m=PTBModel.instance()   \r\n","K = tf.keras.backend \r\n","for i in range(max_epoch):\r\n","    # Define the decay for this epoch\r\n","    lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\r\n","    dcr = learning_rate * lr_decay\r\n","    m._lr = dcr\r\n","    K.set_value(m._model.optimizer.learning_rate,m._lr)\r\n","    print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\r\n","    # Run the loop for this epoch in the training mode\r\n","    train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\r\n","    print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\r\n","        \r\n","    # Run the loop for this epoch in the validation mode\r\n","    valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\r\n","    print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\r\n","    \r\n","# Run the loop in the testing mode to see how effective was our training\r\n","test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\r\n","print(\"Test Perplexity: %.3f\" % test_perplexity)\r\n","\r\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_vocab (Embedding)  (30, 20, 200)             2000000   \n","_________________________________________________________________\n","rnn_1 (RNN)                  (30, 20, 128)             671088    \n","_________________________________________________________________\n","dense_1 (Dense)              (30, 20, 10000)           1290000   \n","_________________________________________________________________\n","activation_1 (Activation)    (30, 20, 10000)           0         \n","=================================================================\n","Total params: 3,961,088\n","Trainable params: 3,955,088\n","Non-trainable params: 6,000\n","_________________________________________________________________\n","Epoch 1 : Learning rate: 1.000\n","Itr 10 of 1549, perplexity: 4602.148 speed: 3109 wps\n","Itr 164 of 1549, perplexity: 1093.977 speed: 3135 wps\n","Itr 318 of 1549, perplexity: 843.383 speed: 3231 wps\n","Itr 472 of 1549, perplexity: 695.995 speed: 3244 wps\n","Itr 626 of 1549, perplexity: 590.558 speed: 3256 wps\n","Itr 780 of 1549, perplexity: 522.963 speed: 3288 wps\n","Itr 934 of 1549, perplexity: 471.133 speed: 3314 wps\n","Itr 1088 of 1549, perplexity: 432.923 speed: 3321 wps\n","Itr 1242 of 1549, perplexity: 403.406 speed: 3344 wps\n","Itr 1396 of 1549, perplexity: 376.037 speed: 3362 wps\n","Epoch 1 : Train Perplexity: 354.461\n","Epoch 1 : Valid Perplexity: 212.609\n","Epoch 2 : Learning rate: 1.000\n","Itr 10 of 1549, perplexity: 232.798 speed: 3073 wps\n","Itr 164 of 1549, perplexity: 209.860 speed: 3174 wps\n","Itr 318 of 1549, perplexity: 201.181 speed: 3106 wps\n","Itr 472 of 1549, perplexity: 192.863 speed: 3110 wps\n","Itr 626 of 1549, perplexity: 184.046 speed: 3162 wps\n","Itr 780 of 1549, perplexity: 180.285 speed: 3202 wps\n","Itr 934 of 1549, perplexity: 176.298 speed: 3261 wps\n","Itr 1088 of 1549, perplexity: 172.975 speed: 3285 wps\n","Itr 1242 of 1549, perplexity: 170.617 speed: 3312 wps\n","Itr 1396 of 1549, perplexity: 166.560 speed: 3342 wps\n","Epoch 2 : Train Perplexity: 163.792\n","Epoch 2 : Valid Perplexity: 163.264\n","Epoch 3 : Learning rate: 1.000\n","Itr 10 of 1549, perplexity: 162.526 speed: 3508 wps\n","Itr 164 of 1549, perplexity: 146.797 speed: 3590 wps\n","Itr 318 of 1549, perplexity: 143.219 speed: 3594 wps\n","Itr 472 of 1549, perplexity: 138.749 speed: 3593 wps\n","Itr 626 of 1549, perplexity: 133.600 speed: 3592 wps\n","Itr 780 of 1549, perplexity: 132.366 speed: 3594 wps\n","Itr 934 of 1549, perplexity: 130.619 speed: 3562 wps\n","Itr 1088 of 1549, perplexity: 129.259 speed: 3560 wps\n","Itr 1242 of 1549, perplexity: 128.412 speed: 3564 wps\n","Itr 1396 of 1549, perplexity: 126.098 speed: 3551 wps\n","Epoch 3 : Train Perplexity: 124.818\n","Epoch 3 : Valid Perplexity: 144.679\n","Epoch 4 : Learning rate: 1.000\n","Itr 10 of 1549, perplexity: 129.617 speed: 3499 wps\n","Itr 164 of 1549, perplexity: 119.801 speed: 3596 wps\n","Itr 318 of 1549, perplexity: 117.774 speed: 3602 wps\n","Itr 472 of 1549, perplexity: 114.449 speed: 3602 wps\n","Itr 626 of 1549, perplexity: 110.579 speed: 3582 wps\n","Itr 780 of 1549, perplexity: 110.126 speed: 3588 wps\n","Itr 934 of 1549, perplexity: 109.087 speed: 3592 wps\n","Itr 1088 of 1549, perplexity: 108.348 speed: 3594 wps\n","Itr 1242 of 1549, perplexity: 107.992 speed: 3596 wps\n","Itr 1396 of 1549, perplexity: 106.300 speed: 3597 wps\n","Epoch 4 : Train Perplexity: 105.532\n","Epoch 4 : Valid Perplexity: 136.266\n","Epoch 5 : Learning rate: 1.000\n","Itr 10 of 1549, perplexity: 111.415 speed: 3578 wps\n","Itr 164 of 1549, perplexity: 104.378 speed: 3600 wps\n","Itr 318 of 1549, perplexity: 102.967 speed: 3607 wps\n","Itr 472 of 1549, perplexity: 100.260 speed: 3499 wps\n","Itr 626 of 1549, perplexity: 97.056 speed: 3436 wps\n","Itr 780 of 1549, perplexity: 96.931 speed: 3400 wps\n","Itr 934 of 1549, perplexity: 96.170 speed: 3375 wps\n","Itr 1088 of 1549, perplexity: 95.699 speed: 3362 wps\n","Itr 1242 of 1549, perplexity: 95.535 speed: 3376 wps\n","Itr 1396 of 1549, perplexity: 94.181 speed: 3375 wps\n","Epoch 5 : Train Perplexity: 93.622\n","Epoch 5 : Valid Perplexity: 133.168\n","Epoch 6 : Learning rate: 0.500\n","Itr 10 of 1549, perplexity: 99.317 speed: 3151 wps\n","Itr 164 of 1549, perplexity: 91.016 speed: 3255 wps\n","Itr 318 of 1549, perplexity: 88.447 speed: 3251 wps\n","Itr 472 of 1549, perplexity: 85.064 speed: 3253 wps\n","Itr 626 of 1549, perplexity: 81.453 speed: 3251 wps\n","Itr 780 of 1549, perplexity: 80.664 speed: 3244 wps\n","Itr 934 of 1549, perplexity: 79.467 speed: 3267 wps\n","Itr 1088 of 1549, perplexity: 78.466 speed: 3266 wps\n","Itr 1242 of 1549, perplexity: 77.731 speed: 3277 wps\n","Itr 1396 of 1549, perplexity: 76.038 speed: 3301 wps\n","Epoch 6 : Train Perplexity: 75.033\n","Epoch 6 : Valid Perplexity: 124.264\n","Epoch 7 : Learning rate: 0.250\n","Itr 10 of 1549, perplexity: 82.675 speed: 3460 wps\n","Itr 164 of 1549, perplexity: 78.061 speed: 3510 wps\n","Itr 318 of 1549, perplexity: 76.016 speed: 3492 wps\n","Itr 472 of 1549, perplexity: 73.089 speed: 3485 wps\n","Itr 626 of 1549, perplexity: 69.893 speed: 3506 wps\n","Itr 780 of 1549, perplexity: 69.175 speed: 3512 wps\n","Itr 934 of 1549, perplexity: 68.076 speed: 3478 wps\n","Itr 1088 of 1549, perplexity: 67.063 speed: 3465 wps\n","Itr 1242 of 1549, perplexity: 66.283 speed: 3487 wps\n","Itr 1396 of 1549, perplexity: 64.698 speed: 3509 wps\n","Epoch 7 : Train Perplexity: 63.682\n","Epoch 7 : Valid Perplexity: 122.537\n","Epoch 8 : Learning rate: 0.125\n","Itr 10 of 1549, perplexity: 74.464 speed: 3659 wps\n","Itr 164 of 1549, perplexity: 70.741 speed: 3707 wps\n","Itr 318 of 1549, perplexity: 69.097 speed: 3705 wps\n","Itr 472 of 1549, perplexity: 66.500 speed: 3705 wps\n","Itr 626 of 1549, perplexity: 63.562 speed: 3706 wps\n","Itr 780 of 1549, perplexity: 62.897 speed: 3707 wps\n","Itr 934 of 1549, perplexity: 61.891 speed: 3709 wps\n","Itr 1088 of 1549, perplexity: 60.921 speed: 3703 wps\n","Itr 1242 of 1549, perplexity: 60.151 speed: 3704 wps\n","Itr 1396 of 1549, perplexity: 58.660 speed: 3704 wps\n","Epoch 8 : Train Perplexity: 57.669\n","Epoch 8 : Valid Perplexity: 121.968\n","Epoch 9 : Learning rate: 0.062\n","Itr 10 of 1549, perplexity: 70.115 speed: 3653 wps\n","Itr 164 of 1549, perplexity: 66.933 speed: 3705 wps\n","Itr 318 of 1549, perplexity: 65.487 speed: 3709 wps\n","Itr 472 of 1549, perplexity: 63.062 speed: 3709 wps\n","Itr 626 of 1549, perplexity: 60.269 speed: 3709 wps\n","Itr 780 of 1549, perplexity: 59.628 speed: 3709 wps\n","Itr 934 of 1549, perplexity: 58.678 speed: 3711 wps\n","Itr 1088 of 1549, perplexity: 57.737 speed: 3711 wps\n","Itr 1242 of 1549, perplexity: 56.973 speed: 3711 wps\n","Itr 1396 of 1549, perplexity: 55.532 speed: 3711 wps\n","Epoch 9 : Train Perplexity: 54.557\n","Epoch 9 : Valid Perplexity: 121.722\n","Epoch 10 : Learning rate: 0.031\n","Itr 10 of 1549, perplexity: 67.876 speed: 3617 wps\n","Itr 164 of 1549, perplexity: 64.924 speed: 234 wps\n","Itr 318 of 1549, perplexity: 63.558 speed: 425 wps\n","Itr 472 of 1549, perplexity: 61.232 speed: 595 wps\n","Itr 626 of 1549, perplexity: 58.521 speed: 747 wps\n","Itr 780 of 1549, perplexity: 57.892 speed: 884 wps\n","Itr 934 of 1549, perplexity: 56.975 speed: 1008 wps\n","Itr 1088 of 1549, perplexity: 56.058 speed: 1120 wps\n","Itr 1242 of 1549, perplexity: 55.296 speed: 1223 wps\n","Itr 1396 of 1549, perplexity: 53.877 speed: 1317 wps\n","Epoch 10 : Train Perplexity: 52.913\n","Epoch 10 : Valid Perplexity: 121.566\n","Epoch 11 : Learning rate: 0.016\n","Itr 10 of 1549, perplexity: 66.769 speed: 3325 wps\n","Itr 164 of 1549, perplexity: 63.854 speed: 3462 wps\n","Itr 318 of 1549, perplexity: 62.515 speed: 3474 wps\n","Itr 472 of 1549, perplexity: 60.234 speed: 3462 wps\n","Itr 626 of 1549, perplexity: 57.573 speed: 3445 wps\n","Itr 780 of 1549, perplexity: 56.949 speed: 3432 wps\n","Itr 934 of 1549, perplexity: 56.049 speed: 3422 wps\n","Itr 1088 of 1549, perplexity: 55.147 speed: 3414 wps\n","Itr 1242 of 1549, perplexity: 54.387 speed: 3405 wps\n","Itr 1396 of 1549, perplexity: 52.981 speed: 3400 wps\n","Epoch 11 : Train Perplexity: 52.021\n","Epoch 11 : Valid Perplexity: 121.425\n","Epoch 12 : Learning rate: 0.008\n","Itr 10 of 1549, perplexity: 66.146 speed: 3268 wps\n","Itr 164 of 1549, perplexity: 63.282 speed: 3374 wps\n","Itr 318 of 1549, perplexity: 61.960 speed: 3375 wps\n","Itr 472 of 1549, perplexity: 59.694 speed: 309 wps\n","Itr 626 of 1549, perplexity: 57.059 speed: 398 wps\n","Itr 780 of 1549, perplexity: 56.438 speed: 483 wps\n","Itr 934 of 1549, perplexity: 55.546 speed: 564 wps\n","Itr 1088 of 1549, perplexity: 54.650 speed: 641 wps\n","Itr 1242 of 1549, perplexity: 53.890 speed: 714 wps\n","Itr 1396 of 1549, perplexity: 52.492 speed: 784 wps\n","Epoch 12 : Train Perplexity: 51.534\n","Epoch 12 : Valid Perplexity: 121.231\n","Epoch 13 : Learning rate: 0.004\n","Itr 10 of 1549, perplexity: 65.780 speed: 3605 wps\n","Itr 164 of 1549, perplexity: 62.965 speed: 3691 wps\n","Itr 318 of 1549, perplexity: 61.659 speed: 3731 wps\n","Itr 472 of 1549, perplexity: 59.403 speed: 3733 wps\n","Itr 626 of 1549, perplexity: 56.781 speed: 3731 wps\n","Itr 780 of 1549, perplexity: 56.163 speed: 3735 wps\n","Itr 934 of 1549, perplexity: 55.274 speed: 3708 wps\n","Itr 1088 of 1549, perplexity: 54.381 speed: 3691 wps\n","Itr 1242 of 1549, perplexity: 53.620 speed: 3676 wps\n","Itr 1396 of 1549, perplexity: 52.228 speed: 3669 wps\n","Epoch 13 : Train Perplexity: 51.272\n","Epoch 13 : Valid Perplexity: 121.009\n","Epoch 14 : Learning rate: 0.002\n","Itr 10 of 1549, perplexity: 65.566 speed: 3590 wps\n","Itr 164 of 1549, perplexity: 62.772 speed: 3630 wps\n","Itr 318 of 1549, perplexity: 61.480 speed: 3619 wps\n","Itr 472 of 1549, perplexity: 59.237 speed: 3635 wps\n","Itr 626 of 1549, perplexity: 56.624 speed: 3642 wps\n","Itr 780 of 1549, perplexity: 56.011 speed: 3637 wps\n","Itr 934 of 1549, perplexity: 55.125 speed: 3637 wps\n","Itr 1088 of 1549, perplexity: 54.234 speed: 3629 wps\n","Itr 1242 of 1549, perplexity: 53.472 speed: 3629 wps\n","Itr 1396 of 1549, perplexity: 52.084 speed: 3630 wps\n","Epoch 14 : Train Perplexity: 51.131\n","Epoch 14 : Valid Perplexity: 120.848\n","Epoch 15 : Learning rate: 0.001\n","Itr 10 of 1549, perplexity: 65.453 speed: 3383 wps\n","Itr 164 of 1549, perplexity: 62.660 speed: 3636 wps\n","Itr 318 of 1549, perplexity: 61.376 speed: 3599 wps\n","Itr 472 of 1549, perplexity: 59.142 speed: 3613 wps\n","Itr 626 of 1549, perplexity: 56.536 speed: 3628 wps\n","Itr 780 of 1549, perplexity: 55.927 speed: 3600 wps\n","Itr 934 of 1549, perplexity: 55.045 speed: 3597 wps\n","Itr 1088 of 1549, perplexity: 54.155 speed: 3608 wps\n","Itr 1242 of 1549, perplexity: 53.393 speed: 3595 wps\n","Itr 1396 of 1549, perplexity: 52.008 speed: 3574 wps\n","Epoch 15 : Train Perplexity: 51.056\n","Epoch 15 : Valid Perplexity: 120.763\n","Test Perplexity: 115.869\n"]}],"metadata":{"tags":[]}},{"cell_type":"markdown","source":["As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower Perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["***\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["This is the end of the <b>Applying Recurrent Neural Networks to Text Processing</b> notebook. Hopefully you now have a better understanding of Recurrent Neural Networks and how to implement one utilizing TensorFlow. Thank you for reading this notebook, and good luck on your studies.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["## Want to learn more?\n","\n","Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets.**Watson Studio** is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies.\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["### Thanks for completing this lesson!\n","\n","Notebook created by <a href=\"https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\">Walter Gomes de Amorim Junior</a>, <a href = \"https://linkedin.com/in/saeedaghabozorgi?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\"> Saeed Aghabozorgi </a></h4>\n","\n","Updated to TF 2.X by  <a href=\"https://www.linkedin.com/in/samaya-madhavan?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01\"> Samaya Madhavan </a>\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}},{"cell_type":"markdown","source":["<hr>\n","\n","Copyright © 2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2021-01-01).\n"],"metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}}}],"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3.9.6 64-bit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"widgets":{"state":{},"version":"1.1.2"},"interpreter":{"hash":"a00727637e30bebd8956149e30c4ae5e5271a9c5969d8e4b0361e9d71d930f15"}},"nbformat":4,"nbformat_minor":4}